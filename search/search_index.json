{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"guis/about/","text":"The OMC GUIs \u00b6 About this Include a foreword about the GUIs, potentially a list doing dispatch like the betabeat one does with its panels. The OMC team develops several GUIs, each for a defined use: The Beta-Beat GUI to perform analysis of measurement files and compute corrections. The Kmod GUI to perform K-modulation in different ways and extract the configuration and results data. The Multiturn GUI to ???. Running the GUIs \u00b6 The GUIs can be started from your development environment or via deployed .jnlp from the archives: Beta-Beat Latest Beta-Beating production version . Latest Beta-Beating development version . Complete list of releases . Kmod Latest Kmod production version . Latest Kmod development version . Complete list of releases . Multiturn Latest Multiturn production version . Latest Multiturn development version . Complete list of releases . Please note that these site are currently available only to devices connected to the CERN network ( workaround ). Open the .jnlp executable inside a browser, or call it with jws from the command line : Compatibility Issues Since javaws (java web start) makes trouble due to intenal security mechanisms, a replacement named jws was developed and has to be used to run the jlnp file. For further information see the jws Confluence page. Requirements \u00b6 The following are required to run the GUIs: A version of Java>=8 . The jws replacement for javaws (in case of errors, see below ). Being inside of the TN is required for the KnobPanel . To do so, either ssh -X to the cs-ccr-dev machines or use the sshuttle method . Kick Groups To make use of the Kick-Groups, your machine needs to have /nfs and /user mounted , like the cs-ccr-dev and cs-ccr-optics machines. Troubleshooting \u00b6 You may encounter the following errors: Problems with execution due to disabled Java \u00b6 If you encounter a complaint about Java being too old, try using /mcr/bin/jws . Failure javaws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp Disabling Java as it is too old and likely to be insecure. To reenable use jcontrol utility Success /mcr/bin/jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp Unspecific Error \u00b6 Failure Any random error If so, check that you can import numpy from the omc-anaconda-python . If this leads to the previously raised error, then the permissions are broken. Either fix the permissions on afs or ask someone to do so for you.","title":"The OMC GUIs"},{"location":"guis/about/#the-omc-guis","text":"About this Include a foreword about the GUIs, potentially a list doing dispatch like the betabeat one does with its panels. The OMC team develops several GUIs, each for a defined use: The Beta-Beat GUI to perform analysis of measurement files and compute corrections. The Kmod GUI to perform K-modulation in different ways and extract the configuration and results data. The Multiturn GUI to ???.","title":"The OMC GUIs"},{"location":"guis/about/#running-the-guis","text":"The GUIs can be started from your development environment or via deployed .jnlp from the archives: Beta-Beat Latest Beta-Beating production version . Latest Beta-Beating development version . Complete list of releases . Kmod Latest Kmod production version . Latest Kmod development version . Complete list of releases . Multiturn Latest Multiturn production version . Latest Multiturn development version . Complete list of releases . Please note that these site are currently available only to devices connected to the CERN network ( workaround ). Open the .jnlp executable inside a browser, or call it with jws from the command line : Compatibility Issues Since javaws (java web start) makes trouble due to intenal security mechanisms, a replacement named jws was developed and has to be used to run the jlnp file. For further information see the jws Confluence page.","title":"Running the GUIs"},{"location":"guis/about/#requirements","text":"The following are required to run the GUIs: A version of Java>=8 . The jws replacement for javaws (in case of errors, see below ). Being inside of the TN is required for the KnobPanel . To do so, either ssh -X to the cs-ccr-dev machines or use the sshuttle method . Kick Groups To make use of the Kick-Groups, your machine needs to have /nfs and /user mounted , like the cs-ccr-dev and cs-ccr-optics machines.","title":"Requirements"},{"location":"guis/about/#troubleshooting","text":"You may encounter the following errors:","title":"Troubleshooting"},{"location":"guis/about/#problems-with-execution-due-to-disabled-java","text":"If you encounter a complaint about Java being too old, try using /mcr/bin/jws . Failure javaws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp Disabling Java as it is too old and likely to be insecure. To reenable use jcontrol utility Success /mcr/bin/jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp","title":"Problems with execution due to disabled Java"},{"location":"guis/about/#unspecific-error","text":"Failure Any random error If so, check that you can import numpy from the omc-anaconda-python . If this leads to the previously raised error, then the permissions are broken. Either fix the permissions on afs or ask someone to do so for you.","title":"Unspecific Error"},{"location":"guis/betabeat/analysis_panel/","text":"The Beta-Beat GUI Analysis Panel \u00b6 The analysis panel provides graphical interface to visualize results from harmonic analysis performed on measured data. The results are given in the tfs format. In the analysis panel one can edit the dp/p value in the corresponding column, and see the changes applied. The Time / Space Tab \u00b6 In the Time / Space tab one can examine the phases and amplitudes, and can clean the values if needed (only TUNEX and TUNEY or NATTUNEX and NATTUNEY ). If some values are obviously not inside a given bound, the 2 marker lines (see screenshot below) can be used to set the boundaries and to remove all data outside those boundaries. The GUI will check if the removal is inside some predefined bounds to prevent accidental removal of too much data. Todo Include a screenshot of the time / space panel with relevant info highlighted (see twiki) The Frequency Tab \u00b6 The Frequency tab displays the computed frequencies for every BPM. A Get Optics button can be used to start the optics calculation. This will call an external python script again, with the results available in the Optics Panel . Todo Include a screenshot of the frequency panel.","title":"Analysis Panel"},{"location":"guis/betabeat/analysis_panel/#the-beta-beat-gui-analysis-panel","text":"The analysis panel provides graphical interface to visualize results from harmonic analysis performed on measured data. The results are given in the tfs format. In the analysis panel one can edit the dp/p value in the corresponding column, and see the changes applied.","title":"The Beta-Beat GUI Analysis Panel"},{"location":"guis/betabeat/analysis_panel/#the-time-space-tab","text":"In the Time / Space tab one can examine the phases and amplitudes, and can clean the values if needed (only TUNEX and TUNEY or NATTUNEX and NATTUNEY ). If some values are obviously not inside a given bound, the 2 marker lines (see screenshot below) can be used to set the boundaries and to remove all data outside those boundaries. The GUI will check if the removal is inside some predefined bounds to prevent accidental removal of too much data. Todo Include a screenshot of the time / space panel with relevant info highlighted (see twiki)","title":"The Time / Space Tab"},{"location":"guis/betabeat/analysis_panel/#the-frequency-tab","text":"The Frequency tab displays the computed frequencies for every BPM. A Get Optics button can be used to start the optics calculation. This will call an external python script again, with the results available in the Optics Panel . Todo Include a screenshot of the frequency panel.","title":"The Frequency Tab"},{"location":"guis/betabeat/beam_selection/","text":"The beam selection Window \u00b6 Todo Include screenshot of Beam-Selection Window and describe the settings. Setting Your Defaults The GUI defaults to specific locations for which Beta-Beat.src directory to use, the input path of you data, etc. Different key-value pairs can be set inside for the desired defaults, for example: beam = LHCB1_RUNII_2018 inputPath = /some/afs/location/with/your/data/ outputPath = /some/afs/location/with/your/results/ betaBeatPath = /some/afs/location/with/your/Beta-Beat.src/ loadData = True These values can be set by a file named bbgui_user.properties (with the above syntax) in either the current working directory, from where you run the GUI, or in your home folder. The latter is only used if there is no such file in the current working directory. If you want to use a specific file located anywhere, you can also give the path to this file as the first and only argument when starting the GUI. It is also possible to set each of these default values by passing them as flag arguments to the GUI call, e.g: --beam LHCB1_RUNII_2018 --inputPath /some/afs/location/with/your/data/ Additional Default Settings Additionally, there are some settings that can only be set via arguments. Their keys and default values are: oldFolderStructure = True consoleLogging = False checkFreeSpace = False oldFolderStructure refers to the folder structure, where the models folder is at top level and contains the machines as subdirectories. Setting this value to False assumes (and creates) the models in a Models folder on the same level as Measurements and Results , i.e. within the machine-folders. consoleLogging activates additional logging into the terminal. The effect is only visible if the GUI was started from the console. checkFreeSpace activates a quick check of the available space upon start of the GUI. The result of this check is logged in the console, and hence also only visible if the GUI is started via terminal command.","title":"Beam Selection"},{"location":"guis/betabeat/beam_selection/#the-beam-selection-window","text":"Todo Include screenshot of Beam-Selection Window and describe the settings. Setting Your Defaults The GUI defaults to specific locations for which Beta-Beat.src directory to use, the input path of you data, etc. Different key-value pairs can be set inside for the desired defaults, for example: beam = LHCB1_RUNII_2018 inputPath = /some/afs/location/with/your/data/ outputPath = /some/afs/location/with/your/results/ betaBeatPath = /some/afs/location/with/your/Beta-Beat.src/ loadData = True These values can be set by a file named bbgui_user.properties (with the above syntax) in either the current working directory, from where you run the GUI, or in your home folder. The latter is only used if there is no such file in the current working directory. If you want to use a specific file located anywhere, you can also give the path to this file as the first and only argument when starting the GUI. It is also possible to set each of these default values by passing them as flag arguments to the GUI call, e.g: --beam LHCB1_RUNII_2018 --inputPath /some/afs/location/with/your/data/ Additional Default Settings Additionally, there are some settings that can only be set via arguments. Their keys and default values are: oldFolderStructure = True consoleLogging = False checkFreeSpace = False oldFolderStructure refers to the folder structure, where the models folder is at top level and contains the machines as subdirectories. Setting this value to False assumes (and creates) the models in a Models folder on the same level as Measurements and Results , i.e. within the machine-folders. consoleLogging activates additional logging into the terminal. The effect is only visible if the GUI was started from the console. checkFreeSpace activates a quick check of the available space upon start of the GUI. The result of this check is logged in the console, and hence also only visible if the GUI is started via terminal command.","title":"The beam selection Window"},{"location":"guis/betabeat/bpm_panel/","text":"The Beta-Beat GUI BPM Panel \u00b6 The BPM panel provides a graphical interface to query and visualize information for the BPM data files. It can load data files for all supported beams, mostly binary SDDS files or files in the SDDS ASCII format. Todo Include a screenshot, possibly of settings when opening files? Opening Files and Applying SVD Cleaning \u00b6 The content of the loaded files will be displayed in two charts: One for the horizontal BPMs, One for the vertical BPMs. Todo Include a screenshot with two BPM panels. The charts are interactive and can be used to zoom in/out, or focus on a given rectangle of the shown data. The charts can display either the measured amplitude values over turns for every BPM from the list or display the phase space, which is calculated by two consecutive BPMs. Additional functionality is done while loading a file. If SVD is enabled in the settings, the external SVD cleaning python script will be called for the current file during the loading process. If SVD cleaning detects and removes bad BPMs, they can be reviewed inside the bad BPM pane. Todo Include a screenshot of the bad bpms panel. Removing Turns and Computing an Average \u00b6 The buttons on the top left side of the pane provide useful features to handle the BPM data. Remove Turns can be used to cut turns from the start or the end, to focus on a specified range of the data. Todo Include a screenshot of before-after comparison for Remove Turns . Create Average allows loading several data files too visualize their average repesentations on the same graph, which helps detecting differences or reducing noise. Todo Include a screenshot of Create Average effect. Do Analysis spawns the configuration dialogue for the external analysis. This will call an external program to perform harmonic analysis of the BPM data, in order to compute tunes and similar beam properties. The results from the analysis can be seen in the Analysis Panel . Todo Include of screenshot of Do Analysis dialogue window. Note The Create Average option requires synchronized data from withing the same bounds, otherwise the results will be meaningless. The figure below shows three runs from LHC beam one with synchronized peaks for every turn and their corresponding averages.","title":"BPM Panel"},{"location":"guis/betabeat/bpm_panel/#the-beta-beat-gui-bpm-panel","text":"The BPM panel provides a graphical interface to query and visualize information for the BPM data files. It can load data files for all supported beams, mostly binary SDDS files or files in the SDDS ASCII format. Todo Include a screenshot, possibly of settings when opening files?","title":"The Beta-Beat GUI BPM Panel"},{"location":"guis/betabeat/bpm_panel/#opening-files-and-applying-svd-cleaning","text":"The content of the loaded files will be displayed in two charts: One for the horizontal BPMs, One for the vertical BPMs. Todo Include a screenshot with two BPM panels. The charts are interactive and can be used to zoom in/out, or focus on a given rectangle of the shown data. The charts can display either the measured amplitude values over turns for every BPM from the list or display the phase space, which is calculated by two consecutive BPMs. Additional functionality is done while loading a file. If SVD is enabled in the settings, the external SVD cleaning python script will be called for the current file during the loading process. If SVD cleaning detects and removes bad BPMs, they can be reviewed inside the bad BPM pane. Todo Include a screenshot of the bad bpms panel.","title":"Opening Files and Applying SVD Cleaning"},{"location":"guis/betabeat/bpm_panel/#removing-turns-and-computing-an-average","text":"The buttons on the top left side of the pane provide useful features to handle the BPM data. Remove Turns can be used to cut turns from the start or the end, to focus on a specified range of the data. Todo Include a screenshot of before-after comparison for Remove Turns . Create Average allows loading several data files too visualize their average repesentations on the same graph, which helps detecting differences or reducing noise. Todo Include a screenshot of Create Average effect. Do Analysis spawns the configuration dialogue for the external analysis. This will call an external program to perform harmonic analysis of the BPM data, in order to compute tunes and similar beam properties. The results from the analysis can be seen in the Analysis Panel . Todo Include of screenshot of Do Analysis dialogue window. Note The Create Average option requires synchronized data from withing the same bounds, otherwise the results will be meaningless. The figure below shows three runs from LHC beam one with synchronized peaks for every turn and their corresponding averages.","title":"Removing Turns and Computing an Average"},{"location":"guis/betabeat/correction_panel/","text":"The Beta-Beat GUI Correction Panel \u00b6 The Correction panel displays the corrections computed from the Optics panel to bring back the measured machine to nominal model conditions. It provides an Open Knob Panel button to access the LHC beam process list. The Knob Panel \u00b6 Through the Knob Panel , corrections can be provided directly inside the LHC beam system. Info Being inside of the Technical Network is required for the KnobPanel. To do so, ssh into one of the hosts, for instance cs-ccr-dev<number>.cern.ch . In the Knob Panel , one can create Knobs (in the Creation tab) by using the previously computed corrections. To create a knob, one or several beam processes have to be selected. Once selected, the corresponding optics will appear. At least one optic has to be selected. After providing a Knob name , the Create Knob button will create a new Knob in the LSA database. Todo Include a screenshot of the Knob Panel on creation tab The View Knobs tab displays a list of all BETA-BEATING Knobs. By selecting one, the user can examine or visualize the values attributed to each component. Todo Include a screenshot of the Knob Panel view knobs table Todo Include a screenshot of the Knob Panel view knobs chart","title":"Correction Panel"},{"location":"guis/betabeat/correction_panel/#the-beta-beat-gui-correction-panel","text":"The Correction panel displays the corrections computed from the Optics panel to bring back the measured machine to nominal model conditions. It provides an Open Knob Panel button to access the LHC beam process list.","title":"The Beta-Beat GUI Correction Panel"},{"location":"guis/betabeat/correction_panel/#the-knob-panel","text":"Through the Knob Panel , corrections can be provided directly inside the LHC beam system. Info Being inside of the Technical Network is required for the KnobPanel. To do so, ssh into one of the hosts, for instance cs-ccr-dev<number>.cern.ch . In the Knob Panel , one can create Knobs (in the Creation tab) by using the previously computed corrections. To create a knob, one or several beam processes have to be selected. Once selected, the corresponding optics will appear. At least one optic has to be selected. After providing a Knob name , the Create Knob button will create a new Knob in the LSA database. Todo Include a screenshot of the Knob Panel on creation tab The View Knobs tab displays a list of all BETA-BEATING Knobs. By selecting one, the user can examine or visualize the values attributed to each component. Todo Include a screenshot of the Knob Panel view knobs table Todo Include a screenshot of the Knob Panel view knobs chart","title":"The Knob Panel"},{"location":"guis/betabeat/gui/","text":"The Beta-Beat GUI \u00b6 The Beta-Beat GUI provides different functionalities separated in multiple views, in the form of a graphical interface. The GUI can be ran locally, provided you have access to afs , but most importantly directly from the CERN Control Center. This section provides a short overview for the main features. The GUI provides several panels, each for a defined use and with a set of options and results: The BPM Panel for loading measurements files, displaying data and launching analysis. The Analysis Panel to display and manipulate the results of harmonic analysis on measurement data. The Optics Panel to compare computed optics to nominal models. The Correction Panel to display the computed necessary corrections to reach the nominal model. This site will guide you through the GUI's layout and functionality. For starters, check out the basics of running the GUI .","title":"About"},{"location":"guis/betabeat/gui/#the-beta-beat-gui","text":"The Beta-Beat GUI provides different functionalities separated in multiple views, in the form of a graphical interface. The GUI can be ran locally, provided you have access to afs , but most importantly directly from the CERN Control Center. This section provides a short overview for the main features. The GUI provides several panels, each for a defined use and with a set of options and results: The BPM Panel for loading measurements files, displaying data and launching analysis. The Analysis Panel to display and manipulate the results of harmonic analysis on measurement data. The Optics Panel to compare computed optics to nominal models. The Correction Panel to display the computed necessary corrections to reach the nominal model. This site will guide you through the GUI's layout and functionality. For starters, check out the basics of running the GUI .","title":"The Beta-Beat GUI"},{"location":"guis/betabeat/omc3/","text":"Beta-Beat GUI for omc3 \u00b6 This page is meant for people, who know the old Gui and want to use the omc3 functionality in the GUI now! For now, this is still in beta and has a lot of not-yet-implemented functionality. On the up-side, it also has some new-and-improved functionality over the Beta-Beat-Source ( master ) branch. The basic optics analysis and plotting work! Important changes \u00b6 Python \u00b6 In the Beam-Selection Window you need to give it a python-binary (e.g. venv/bin/python ). Best would be if you have a local virtual environment. This needs to have omc3 installed as a package . See omc3 on github . pip install git+https://github.com/pylhc/omc3.git This is because python calls are now made by module, i.e.: python -m omc3.module arg1 arg2 ... Settings \u00b6 Settings are now all in one place (the settings-button on top). Entries in the settings that are lists (e.g. 'Turns' which will be STARTTURN ENDTURN ) are given as space-separated values, NOT comma separated. Settings can be reverted as long as you do not click apply. Settings are applied automatically on OK . Settings are reset to last applied on Cancel . Opening Files \u00b6 Each tab has now an Open Files button, which opens only the files specific to this tab. The magic + button is gone, as its functionality was confusing (and there were different stories about its workings). Optics Plotting \u00b6 RDT and CRDT plots are added dynamically depending on the files present in the respective folders. Nicer names and more structure in the tree. Backend was rewritten, so it is now more modular and easier to add new plot-types. Keys in Plots \u00b6 Left Button : Draw and zoom into rectangle. Middle Button : Auto-zoom (in 3 Steps: 4\u03c3, 3\u03c3, 2\u03c3). Right Button : Undo last step. Shift + Right Button : Undo all steps (reset plot). Mouse Wheel : Zoom relative to mouse position. Shift + Mouse Wheel : Zoom y-axis relative to mouse position. Alt + Mouse Wheel : Zoom relative to plot center. Shift + Alt + Mouse Wheel : Zoom y-axis relative to plot center. Nattune Updater \u00b6 You can set a frequency range and it does not redo the analysis but just picks the highest peak in that range and assigns it to NATTUNE in the lin-file. This should be very helpful for amplitude detuning analysis. Known not to work: \u00b6 Other accelerators than the LHC (you can trick it a bit, though, by choosing LHC and then changing the accelerator manually in the settings ). Removing turns (but you can set Start and End turn in the settings). BBQ compensation for amplitude detuning can't be called directly from the GUI. Things that do not work as they are not implemented in omc3 : Global Corrections, SegmentBySegment, Spectrum Plot Export cannot export both planes into one axis, Optics Plot Export cannot have separate limits for the two plots, Optics Plot Export cannot export CRDT plots, Do NOT use the Nattune-Updater (in the Spectrum panel) if you have free kicks (it adds a NATTUNE -Column to the lin-file). Bug Reporting If you find bugs, please make JIRA tickets with the OMC3-GUI label.","title":"The omc3 branch"},{"location":"guis/betabeat/omc3/#beta-beat-gui-for-omc3","text":"This page is meant for people, who know the old Gui and want to use the omc3 functionality in the GUI now! For now, this is still in beta and has a lot of not-yet-implemented functionality. On the up-side, it also has some new-and-improved functionality over the Beta-Beat-Source ( master ) branch. The basic optics analysis and plotting work!","title":"Beta-Beat GUI for omc3"},{"location":"guis/betabeat/omc3/#important-changes","text":"","title":"Important changes"},{"location":"guis/betabeat/omc3/#python","text":"In the Beam-Selection Window you need to give it a python-binary (e.g. venv/bin/python ). Best would be if you have a local virtual environment. This needs to have omc3 installed as a package . See omc3 on github . pip install git+https://github.com/pylhc/omc3.git This is because python calls are now made by module, i.e.: python -m omc3.module arg1 arg2 ...","title":"Python"},{"location":"guis/betabeat/omc3/#settings","text":"Settings are now all in one place (the settings-button on top). Entries in the settings that are lists (e.g. 'Turns' which will be STARTTURN ENDTURN ) are given as space-separated values, NOT comma separated. Settings can be reverted as long as you do not click apply. Settings are applied automatically on OK . Settings are reset to last applied on Cancel .","title":"Settings"},{"location":"guis/betabeat/omc3/#opening-files","text":"Each tab has now an Open Files button, which opens only the files specific to this tab. The magic + button is gone, as its functionality was confusing (and there were different stories about its workings).","title":"Opening Files"},{"location":"guis/betabeat/omc3/#optics-plotting","text":"RDT and CRDT plots are added dynamically depending on the files present in the respective folders. Nicer names and more structure in the tree. Backend was rewritten, so it is now more modular and easier to add new plot-types.","title":"Optics Plotting"},{"location":"guis/betabeat/omc3/#keys-in-plots","text":"Left Button : Draw and zoom into rectangle. Middle Button : Auto-zoom (in 3 Steps: 4\u03c3, 3\u03c3, 2\u03c3). Right Button : Undo last step. Shift + Right Button : Undo all steps (reset plot). Mouse Wheel : Zoom relative to mouse position. Shift + Mouse Wheel : Zoom y-axis relative to mouse position. Alt + Mouse Wheel : Zoom relative to plot center. Shift + Alt + Mouse Wheel : Zoom y-axis relative to plot center.","title":"Keys in Plots"},{"location":"guis/betabeat/omc3/#nattune-updater","text":"You can set a frequency range and it does not redo the analysis but just picks the highest peak in that range and assigns it to NATTUNE in the lin-file. This should be very helpful for amplitude detuning analysis.","title":"Nattune Updater"},{"location":"guis/betabeat/omc3/#known-not-to-work","text":"Other accelerators than the LHC (you can trick it a bit, though, by choosing LHC and then changing the accelerator manually in the settings ). Removing turns (but you can set Start and End turn in the settings). BBQ compensation for amplitude detuning can't be called directly from the GUI. Things that do not work as they are not implemented in omc3 : Global Corrections, SegmentBySegment, Spectrum Plot Export cannot export both planes into one axis, Optics Plot Export cannot have separate limits for the two plots, Optics Plot Export cannot export CRDT plots, Do NOT use the Nattune-Updater (in the Spectrum panel) if you have free kicks (it adds a NATTUNE -Column to the lin-file). Bug Reporting If you find bugs, please make JIRA tickets with the OMC3-GUI label.","title":"Known not to work:"},{"location":"guis/betabeat/optics_panel/","text":"The Beta-Beat GUI Optics Panel \u00b6 The Optics Panel provides graphical interface to compare the computed optics to the nominal model. There are in total three main tabs for the optics panel: The Optics tab, where a tree menu (on the left) provides many physical properties to be displayed. The Segment-by-Segment: Segment tab, to have a look at properties in a pre-defined segment of the machine. The Segment-by-Segment: Element tab, to have a look at properties for a pre-defined list of elements in the machine. Optics Tab \u00b6 By default, the user is taken to the Optics tab. A wide variety of computed physical properties can be visualized across the entire machine. Todo Include a screenshot with the main optics tab. Segment-by-Segment: Segment Tab \u00b6 A list of pre-defined segments of the machine can be selected to view properties across said segment. Todo Include a screenshot of segment selection. In the event that one wants to visualize a specific, non pre-defined segment, it is possible to create a new one. To do so: In the Optics tab, pick the start BPM by clicking on a BPM point on the optics chart. Pick the end BPM by clicking on another BPM point. A pop-up appears in which the name of the new segment has to be entered. Todo Include a screenshot of the new segment creation dialogue. Clicking Go will call for another python script and take you to the Segment-by-Segment tab to view the results. Segment-by-Segment: Element Tab \u00b6 Pre-defined lists of elements can be selected to view properties across said elements. The working is similar to the one for different segments . Todo Include a screenshot of the element tab. Computing Corrections \u00b6 The Correction button at the bottom left of the optics panel can be used to calculate the optics corrections for the beam process. The settings dialogue offers a wide range of different options for corrections. This will call different external python scripts again. These scripts calculate corrections for beta-beat, coupling and horizontal and vertical dispersion using the computed response matrices. The following methods implement different correction algorithms: Coupling : Single beam correction of coupling resonances and vertical dispersion. Global correction : Single beam correction of phase, beta and horizontal dispersion. Iterative correction : Two-beams version of the global correction. Chromatic coupling : Single beam correction of chromatic coupling using skew sextupoles at dispersive locations. Note The Iterative correction method is currently not compatible and thus disabled. The results are outputted in the changeparameters files. These files store the magnet names and corresponding correction strengths. They are also displayed in the Correction Panel .","title":"Optics Panel"},{"location":"guis/betabeat/optics_panel/#the-beta-beat-gui-optics-panel","text":"The Optics Panel provides graphical interface to compare the computed optics to the nominal model. There are in total three main tabs for the optics panel: The Optics tab, where a tree menu (on the left) provides many physical properties to be displayed. The Segment-by-Segment: Segment tab, to have a look at properties in a pre-defined segment of the machine. The Segment-by-Segment: Element tab, to have a look at properties for a pre-defined list of elements in the machine.","title":"The Beta-Beat GUI Optics Panel"},{"location":"guis/betabeat/optics_panel/#optics-tab","text":"By default, the user is taken to the Optics tab. A wide variety of computed physical properties can be visualized across the entire machine. Todo Include a screenshot with the main optics tab.","title":"Optics Tab"},{"location":"guis/betabeat/optics_panel/#segment-by-segment-segment-tab","text":"A list of pre-defined segments of the machine can be selected to view properties across said segment. Todo Include a screenshot of segment selection. In the event that one wants to visualize a specific, non pre-defined segment, it is possible to create a new one. To do so: In the Optics tab, pick the start BPM by clicking on a BPM point on the optics chart. Pick the end BPM by clicking on another BPM point. A pop-up appears in which the name of the new segment has to be entered. Todo Include a screenshot of the new segment creation dialogue. Clicking Go will call for another python script and take you to the Segment-by-Segment tab to view the results.","title":"Segment-by-Segment: Segment Tab"},{"location":"guis/betabeat/optics_panel/#segment-by-segment-element-tab","text":"Pre-defined lists of elements can be selected to view properties across said elements. The working is similar to the one for different segments . Todo Include a screenshot of the element tab.","title":"Segment-by-Segment: Element Tab"},{"location":"guis/betabeat/optics_panel/#computing-corrections","text":"The Correction button at the bottom left of the optics panel can be used to calculate the optics corrections for the beam process. The settings dialogue offers a wide range of different options for corrections. This will call different external python scripts again. These scripts calculate corrections for beta-beat, coupling and horizontal and vertical dispersion using the computed response matrices. The following methods implement different correction algorithms: Coupling : Single beam correction of coupling resonances and vertical dispersion. Global correction : Single beam correction of phase, beta and horizontal dispersion. Iterative correction : Two-beams version of the global correction. Chromatic coupling : Single beam correction of chromatic coupling using skew sextupoles at dispersive locations. Note The Iterative correction method is currently not compatible and thus disabled. The results are outputted in the changeparameters files. These files store the magnet names and corresponding correction strengths. They are also displayed in the Correction Panel .","title":"Computing Corrections"},{"location":"guis/developing/ide_install/","text":"IDE Setup \u00b6 The easiest way to develop the GUI is using the modified Eclipse versions provided by Accsoft-Eclipse . There is also an installation guide available . For people using pycharm it might make sense to use IntelliJ IDEA, however we do not yet know how to export a JAR (see also GUI Releases ). As both IDEs require CommonBuildNextGeneration (CBNG) to resolve dependencies and make releases, one should either run these from somewhere in the Technical Network (e.g. from the dev-server ) or mount the required paths via sshfs as described here . Installation \u00b6 Eclipse Download your preferred version from their download page and install. With this version, CBNG comes automatically installed and can be used by simply dragging the desired project into the CBNG window . For more info see the accsoft eclipse wiki . IntelliJ IDEA Download your preferred version from their download page and install. CBNG needs to be setup in IDEA manually, by setting the Gradle home path in: File -> Settings -> Build, Execution, Deployment -> Build Tools -> Gradle to the specified location /user/pcrops/devtools/CBNG/PRO/ A more extensive guide can be found in the CBNG Wiki for IDEA integration . Alternatively, run CBNG directly from the commandline instead. CBNG from the Command Line Another sometimes easier alternative that works with any IDE is to run CBNGs bob with the desired command (e.g. build , dependencies , eclipse , idea ) in the folder of the project from the commandline. The full path to bob is: /user/pcrops/devtools/CBNG/PRO/bin/bob See bob --help for instructions about its commands. Importing a Project \u00b6 The project can be imported using the git-integrations of the IDEs directly, using the Gitlab paths below. This should be straightforward, but you are giving up some control. In the next subsections the manual path of getting the source-code into the IDE is outlined in the hope to point to some pitfalls that may occur and how to avoid them. Firstly, you should clone the desired repository to an adequate location on your hard-drive, depending on which project you want to work on: Beta-Beat GUI git clone https://gitlab.cern.ch/acc-co/lhc/lhc-app-beta-beating Kmod GUI git clone https://gitlab.cern.ch/acc-co/lhc/lhc-app-kmod You then simply import the project into your IDE. IntelliJ Specificity For IntelliJ, you might have to Create an empty build.gradle file if you want to trigger a gradle import dialogue where you need to choose use local gradle distribution and set the gradle home to /user/pcrops/devtools/CBNG/PRO/bin/bob ( as above ) Go to File -> Project Structure ... -> Modules and set the Dependencies storage format to Eclipse (.classpath) . This one you should check on a regular basis, as it tends to reset itself. To make it runnable, you will have to use CBNG to resolve dependencies and build the project first. Depending on your IDE you should run CBNGs eclipse or idea followed by build . Running dependencies can help. No one in the OMC-Team is a CBNG expert, and sometimes running these commands leads to the desired outcome (of a runnable project) or not depending on the color of the DG 's clothing. Running the GUI \u00b6 If everything worked fine, the Gui should now be runnable via the void main() function in the main.Main.java , which can be invoked by right-clicking on main.Main and choosing Run or manually setting a Java Run configuration in the Run menu. Useful Links \u00b6 CBNG Wiki Accsoft-Eclipse Downloads Accsoft Eclipse Wiki","title":"IDE Setup"},{"location":"guis/developing/ide_install/#ide-setup","text":"The easiest way to develop the GUI is using the modified Eclipse versions provided by Accsoft-Eclipse . There is also an installation guide available . For people using pycharm it might make sense to use IntelliJ IDEA, however we do not yet know how to export a JAR (see also GUI Releases ). As both IDEs require CommonBuildNextGeneration (CBNG) to resolve dependencies and make releases, one should either run these from somewhere in the Technical Network (e.g. from the dev-server ) or mount the required paths via sshfs as described here .","title":"IDE Setup"},{"location":"guis/developing/ide_install/#installation","text":"Eclipse Download your preferred version from their download page and install. With this version, CBNG comes automatically installed and can be used by simply dragging the desired project into the CBNG window . For more info see the accsoft eclipse wiki . IntelliJ IDEA Download your preferred version from their download page and install. CBNG needs to be setup in IDEA manually, by setting the Gradle home path in: File -> Settings -> Build, Execution, Deployment -> Build Tools -> Gradle to the specified location /user/pcrops/devtools/CBNG/PRO/ A more extensive guide can be found in the CBNG Wiki for IDEA integration . Alternatively, run CBNG directly from the commandline instead. CBNG from the Command Line Another sometimes easier alternative that works with any IDE is to run CBNGs bob with the desired command (e.g. build , dependencies , eclipse , idea ) in the folder of the project from the commandline. The full path to bob is: /user/pcrops/devtools/CBNG/PRO/bin/bob See bob --help for instructions about its commands.","title":"Installation"},{"location":"guis/developing/ide_install/#importing-a-project","text":"The project can be imported using the git-integrations of the IDEs directly, using the Gitlab paths below. This should be straightforward, but you are giving up some control. In the next subsections the manual path of getting the source-code into the IDE is outlined in the hope to point to some pitfalls that may occur and how to avoid them. Firstly, you should clone the desired repository to an adequate location on your hard-drive, depending on which project you want to work on: Beta-Beat GUI git clone https://gitlab.cern.ch/acc-co/lhc/lhc-app-beta-beating Kmod GUI git clone https://gitlab.cern.ch/acc-co/lhc/lhc-app-kmod You then simply import the project into your IDE. IntelliJ Specificity For IntelliJ, you might have to Create an empty build.gradle file if you want to trigger a gradle import dialogue where you need to choose use local gradle distribution and set the gradle home to /user/pcrops/devtools/CBNG/PRO/bin/bob ( as above ) Go to File -> Project Structure ... -> Modules and set the Dependencies storage format to Eclipse (.classpath) . This one you should check on a regular basis, as it tends to reset itself. To make it runnable, you will have to use CBNG to resolve dependencies and build the project first. Depending on your IDE you should run CBNGs eclipse or idea followed by build . Running dependencies can help. No one in the OMC-Team is a CBNG expert, and sometimes running these commands leads to the desired outcome (of a runnable project) or not depending on the color of the DG 's clothing.","title":"Importing a Project"},{"location":"guis/developing/ide_install/#running-the-gui","text":"If everything worked fine, the Gui should now be runnable via the void main() function in the main.Main.java , which can be invoked by right-clicking on main.Main and choosing Run or manually setting a Java Run configuration in the Run menu.","title":"Running the GUI"},{"location":"guis/developing/ide_install/#useful-links","text":"CBNG Wiki Accsoft-Eclipse Downloads Accsoft Eclipse Wiki","title":"Useful Links"},{"location":"guis/developing/kmod_special/","text":"Special Features of the KMod GUI Development \u00b6 Analyzing Tasks of the AutoTrim \u00b6 Analyzing tasks are executed after the trimming task of one KmodProcess . The list of analyzing tasks can hold as many tasks as needed since they are getting executed in the background while another process is trimming. The results are displayed to the corresponding tab on the result panel. Most of the time each analyzing task represents one python script which is called from the GUI. Todo KMod specific Python analysing tools. Mention to transform to Python3? Link on how to add external programs (BBGUI).","title":"KMod GUI hints"},{"location":"guis/developing/kmod_special/#special-features-of-the-kmod-gui-development","text":"","title":"Special Features of the KMod GUI Development"},{"location":"guis/developing/kmod_special/#analyzing-tasks-of-the-autotrim","text":"Analyzing tasks are executed after the trimming task of one KmodProcess . The list of analyzing tasks can hold as many tasks as needed since they are getting executed in the background while another process is trimming. The results are displayed to the corresponding tab on the result panel. Most of the time each analyzing task represents one python script which is called from the GUI. Todo KMod specific Python analysing tools. Mention to transform to Python3? Link on how to add external programs (BBGUI).","title":"Analyzing Tasks of the AutoTrim"},{"location":"guis/developing/releases/","text":"Software Releases \u00b6","title":"Releases"},{"location":"guis/developing/releases/#software-releases","text":"","title":"Software Releases"},{"location":"guis/kmod/autotrim/","text":"Auto Trim \u00b6 The AutoTrim functionality combines all existing tasks in the Kmod GUI. The user can choose tasks to do for each IP . The AutoTrim then executes all the trimming tasks and analyzing tasks in the best order possible to optimize the process. The user can input parameters once to do the k-modulation and other analysis tasks on many IP 's. The results and current status of each process is displayed on the main panel. The AutoTrim provides something like a pipeline of many k-modulations and their analyzing tasks. First Time AutoTrim If you are in the CCC and have never used the AutoTrim before, follow these steps: Open the AutoTrimDialog by clicking on the AutoTim button located on the left side of the main panel. Add the IPs you want to trim or analyze with the buttons on the left side. Input the trim parameter or load an existing trim for each IP tab. Select the analyzing options you want to execute. Input the needed values for the trim and all the optional tasks. Once everything is set up, right press the Start button and select the directory where to save the results (It will save the results depending on the trim time and IP . See wiki for more information). The AutoTrim will now check if the orbit feedback is on and the tune feedback is off. Make sure to put them in the desired state. The ResultsView will open and the AutoTrim gets started. AutoTrim Panel \u00b6 The InputPanel \u00b6 This input panel provides all the options to set every parameter for k-modulation. On the left panel the user can choose which IP to trim or analyze. Depending on the parameters, the panel will show input panels for the specific IPs: each panel has a top bar with optional tasks that can be executed after the trim as well as an input field for the current energy. The energy value is gathered live from the LHC, but can be edited. Underneath is another tabbed pane for each task in the current process which needs some user input. The AutoTrim input panel. By pressing the start button, it will ask the user for the save directory and then starts the AutoTrim process. AutoTrim will then create a result folder for each IP , open the ResultsView on the main panel and start the pipeline of processes. The chosen save directory is the root directory. The AutoTrim will then create a subdirectory for each IP where the result files are stored. In principle, this will perform a full IP trim for all of the selected IPs and planes. For more details, see the full IP trim section. The ResultsView \u00b6 The ResultsView is the main panel of an AutoTrim process. It is built as a tabbed pane which holds the content of the processes for many IPs and displays information such as process status, results, etc. The status panel and the result panel are the main components for each process. The ResultsView. At the top is a thread queue bar which displays all the active, finished and scheduled threads. The thread queue of the results view. Tip By double-clicking on the IP label of the tabbed pane its possible to extract the IP panel in another frame. It will snap back to the tabbed pane if it gets closed by the user. The Status Panel \u00b6 The status panel shows information about the current process, with the currently running task is displayed at the top. The user can click through the list of tasks to see start/end time, input parameters, log entries, and the current status. There is also a panel for the input parameter of thes current task. This has to be done in the actual task code since the view doesn't have a reference to the task. The last panel is the logger text area for each task. Per default, it will show the last logging entry with the corresponding time, but the user can expand the list by clicking on the drop-down button. The AutoTrim Status Panel. Status Types Explained Status Description Starting Task is creating all the needed objects or loads some data from a file.* Running Main functionality of this task is currently executed. Finished Task finished without any problems. Waiting Task is waiting for some synchronization variable.* Scheduled Task is in the queue and ready to be executed. Canceled Task got canceled by the user (Not possible still a big TODO). Crashed Task crashed because an exception is fired or a critical error occurred. * Not used at the moment, might be useful later (non thread-safe functionality, etc.) Loading Existing Trims from Timber \u00b6 The AutoTrim provides two ways to add existing trim data to the process queue: Adding the selected Timber entry directly to an existing IP tab in the AutoTrim panel. There, one can only select the trim entries depending on the IP of the tab. Selecting any trim you want with the Trim-Selection panel, which will then add the selected trims as tabs to the AutoTrim panel. Load a trim directly to an AutoTrim-Tab \u00b6 Start by creating your IP tab by clicking on the button on the left side: tick the Load existing trim checkbox and it will open the trim selection panel. AutoTrim: load existing trim. Expand the date picker and choose the day when the trim was executed. Left click on the trim you want to load and press the Select trim button. AutoTrim: load existing trim, date picker. The loaded trim date is then displayed on the trim panel and all the other input fields for the Full IP Trim are disabled, as input is not needed. To discard the loaded trim and proceed with the Full IP Trim instead, untick the Load existing trim checkbox. The Trim-Selection Panel \u00b6 The Trim-Selection panel will add the selected trim data as tabs to the AutoTrim panel. It also provides some basic options to apply to every tab, such as setting analyzing the orbit to default. This might be the best option if you want to reanalyze many trims with the same model. Open this panel by pressing the Load Trims button located on the bottom center of the AutoTrim panel. Filter the Timber entries by IP and date, then select the trim entries in the Search Results panel and press the green Add selected trim button. The trims are now selected and displayed in the Selected Trims panel. By clicking on confirm the tabs will be added to the AutoTrim panel. You can now set some basic options for the selected trims in the Options pane. These values are automatically set as preset values for the loaded trim tabs. The AutoTrim Trim Selection Wizard. Simulation Mode \u00b6 The kmod application provides two different modes to execute all its functionality: The Measurement mode , which is the standard mode and should be used in the control center to gather real data from the LHC. The Simulation mode , which only simulates these live measurements. If you start a normal trim task in Simulation mode the AutoTrim will load the last trim data from timber. This data is then displayed like a normal trim during a live measurement. The energy value is not automatically loaded from Timber, and uses a preset value of 6500.0 GeV. The AutoTrim won't check if the Orbit and Tune feedback is on or off. Analyzing a Trim \u00b6 Input Tab \u00b6 This input tab is separated into two sides, one for each existing beam in the LHC. One side holds the input fields for all needed parameters in order to run the beta star analysis script. It is also possible to provide only one estimated beta star value as input by ticking the Use round optics option. This checkbox will disable the input for the beta star in the vertical plane and uses the horizontal input for both. Parameters: B* X [m] : Estimated horizontal beta-star (X) of measurements as a double. B* Y [m] : Estimated vertical beta-star (Y) of measurements as a double. Waist Shift [m] : Estimated waist-shift as double value. The Analyzing Configuration. Results Panel \u00b6 This analyzing task will display the results on the Analyzing Trim tab. It will list the \\(\\beta^{*}\\) values and waist shift on the horizontal and vertical plane for each beam. The saved plots of the script are displayed on the right side for each beam. By clicking on the image it will expand and show the full-size plot in a new frame. The Analyzing Trim Results.","title":"Using Auto Trim"},{"location":"guis/kmod/autotrim/#auto-trim","text":"The AutoTrim functionality combines all existing tasks in the Kmod GUI. The user can choose tasks to do for each IP . The AutoTrim then executes all the trimming tasks and analyzing tasks in the best order possible to optimize the process. The user can input parameters once to do the k-modulation and other analysis tasks on many IP 's. The results and current status of each process is displayed on the main panel. The AutoTrim provides something like a pipeline of many k-modulations and their analyzing tasks. First Time AutoTrim If you are in the CCC and have never used the AutoTrim before, follow these steps: Open the AutoTrimDialog by clicking on the AutoTim button located on the left side of the main panel. Add the IPs you want to trim or analyze with the buttons on the left side. Input the trim parameter or load an existing trim for each IP tab. Select the analyzing options you want to execute. Input the needed values for the trim and all the optional tasks. Once everything is set up, right press the Start button and select the directory where to save the results (It will save the results depending on the trim time and IP . See wiki for more information). The AutoTrim will now check if the orbit feedback is on and the tune feedback is off. Make sure to put them in the desired state. The ResultsView will open and the AutoTrim gets started.","title":"Auto Trim"},{"location":"guis/kmod/autotrim/#autotrim-panel","text":"","title":"AutoTrim Panel"},{"location":"guis/kmod/autotrim/#the-inputpanel","text":"This input panel provides all the options to set every parameter for k-modulation. On the left panel the user can choose which IP to trim or analyze. Depending on the parameters, the panel will show input panels for the specific IPs: each panel has a top bar with optional tasks that can be executed after the trim as well as an input field for the current energy. The energy value is gathered live from the LHC, but can be edited. Underneath is another tabbed pane for each task in the current process which needs some user input. The AutoTrim input panel. By pressing the start button, it will ask the user for the save directory and then starts the AutoTrim process. AutoTrim will then create a result folder for each IP , open the ResultsView on the main panel and start the pipeline of processes. The chosen save directory is the root directory. The AutoTrim will then create a subdirectory for each IP where the result files are stored. In principle, this will perform a full IP trim for all of the selected IPs and planes. For more details, see the full IP trim section.","title":"The InputPanel"},{"location":"guis/kmod/autotrim/#the-resultsview","text":"The ResultsView is the main panel of an AutoTrim process. It is built as a tabbed pane which holds the content of the processes for many IPs and displays information such as process status, results, etc. The status panel and the result panel are the main components for each process. The ResultsView. At the top is a thread queue bar which displays all the active, finished and scheduled threads. The thread queue of the results view. Tip By double-clicking on the IP label of the tabbed pane its possible to extract the IP panel in another frame. It will snap back to the tabbed pane if it gets closed by the user.","title":"The ResultsView"},{"location":"guis/kmod/autotrim/#the-status-panel","text":"The status panel shows information about the current process, with the currently running task is displayed at the top. The user can click through the list of tasks to see start/end time, input parameters, log entries, and the current status. There is also a panel for the input parameter of thes current task. This has to be done in the actual task code since the view doesn't have a reference to the task. The last panel is the logger text area for each task. Per default, it will show the last logging entry with the corresponding time, but the user can expand the list by clicking on the drop-down button. The AutoTrim Status Panel. Status Types Explained Status Description Starting Task is creating all the needed objects or loads some data from a file.* Running Main functionality of this task is currently executed. Finished Task finished without any problems. Waiting Task is waiting for some synchronization variable.* Scheduled Task is in the queue and ready to be executed. Canceled Task got canceled by the user (Not possible still a big TODO). Crashed Task crashed because an exception is fired or a critical error occurred. * Not used at the moment, might be useful later (non thread-safe functionality, etc.)","title":"The Status Panel"},{"location":"guis/kmod/autotrim/#loading-existing-trims-from-timber","text":"The AutoTrim provides two ways to add existing trim data to the process queue: Adding the selected Timber entry directly to an existing IP tab in the AutoTrim panel. There, one can only select the trim entries depending on the IP of the tab. Selecting any trim you want with the Trim-Selection panel, which will then add the selected trims as tabs to the AutoTrim panel.","title":"Loading Existing Trims from Timber"},{"location":"guis/kmod/autotrim/#load-a-trim-directly-to-an-autotrim-tab","text":"Start by creating your IP tab by clicking on the button on the left side: tick the Load existing trim checkbox and it will open the trim selection panel. AutoTrim: load existing trim. Expand the date picker and choose the day when the trim was executed. Left click on the trim you want to load and press the Select trim button. AutoTrim: load existing trim, date picker. The loaded trim date is then displayed on the trim panel and all the other input fields for the Full IP Trim are disabled, as input is not needed. To discard the loaded trim and proceed with the Full IP Trim instead, untick the Load existing trim checkbox.","title":"Load a trim directly to an AutoTrim-Tab"},{"location":"guis/kmod/autotrim/#the-trim-selection-panel","text":"The Trim-Selection panel will add the selected trim data as tabs to the AutoTrim panel. It also provides some basic options to apply to every tab, such as setting analyzing the orbit to default. This might be the best option if you want to reanalyze many trims with the same model. Open this panel by pressing the Load Trims button located on the bottom center of the AutoTrim panel. Filter the Timber entries by IP and date, then select the trim entries in the Search Results panel and press the green Add selected trim button. The trims are now selected and displayed in the Selected Trims panel. By clicking on confirm the tabs will be added to the AutoTrim panel. You can now set some basic options for the selected trims in the Options pane. These values are automatically set as preset values for the loaded trim tabs. The AutoTrim Trim Selection Wizard.","title":"The Trim-Selection Panel"},{"location":"guis/kmod/autotrim/#simulation-mode","text":"The kmod application provides two different modes to execute all its functionality: The Measurement mode , which is the standard mode and should be used in the control center to gather real data from the LHC. The Simulation mode , which only simulates these live measurements. If you start a normal trim task in Simulation mode the AutoTrim will load the last trim data from timber. This data is then displayed like a normal trim during a live measurement. The energy value is not automatically loaded from Timber, and uses a preset value of 6500.0 GeV. The AutoTrim won't check if the Orbit and Tune feedback is on or off.","title":"Simulation Mode"},{"location":"guis/kmod/autotrim/#analyzing-a-trim","text":"","title":"Analyzing a Trim"},{"location":"guis/kmod/autotrim/#input-tab","text":"This input tab is separated into two sides, one for each existing beam in the LHC. One side holds the input fields for all needed parameters in order to run the beta star analysis script. It is also possible to provide only one estimated beta star value as input by ticking the Use round optics option. This checkbox will disable the input for the beta star in the vertical plane and uses the horizontal input for both. Parameters: B* X [m] : Estimated horizontal beta-star (X) of measurements as a double. B* Y [m] : Estimated vertical beta-star (Y) of measurements as a double. Waist Shift [m] : Estimated waist-shift as double value. The Analyzing Configuration.","title":"Input Tab"},{"location":"guis/kmod/autotrim/#results-panel","text":"This analyzing task will display the results on the Analyzing Trim tab. It will list the \\(\\beta^{*}\\) values and waist shift on the horizontal and vertical plane for each beam. The saved plots of the script are displayed on the right side for each beam. By clicking on the image it will expand and show the full-size plot in a new frame. The Analyzing Trim Results.","title":"Results Panel"},{"location":"guis/kmod/full_ip_trim/","text":"Performing a Full IP Trim \u00b6 The Full IP Trim module of the GUI executes the K-Modulation on a selected Interaction Point with its predefined quadrupoles. Each bigger experiment like ATLAS (IP1), ALICE (IP2), CMS (IP5) and LHCb (IP8) is referred to by its corresponding number in the LHC complex. Choose an IP. By using this module, the quadrupole identifier and several other parameters are predefined to the corresponding Interaction Points. After the modulation is done, the trim data is present on the database and can be visualized and analyzed using the measurement view dialog. In order to start the K-Modulation on a magnet, the power converter has to be accessed using the CERN provided LSA Java API. The available parameters are the following: Delta I : Change of magnet strength in Amperes. Default value is 6.0A . Frequency : How fast the magnet strength is changed. Default value is 0.013 . Cycles : How many times the modulation is done. Default value is 3 . Select trim parameter. Result Panel \u00b6 While the Full IP Trim is executed the current and the tune values are displayed live in the result panel. After the modulation is done, it will show the whole trim results in the corresponding ResultPanel of this trimming task. The trim results are stored in the save directory of the current IP . Trim result panel.","title":"Full IP Trim"},{"location":"guis/kmod/full_ip_trim/#performing-a-full-ip-trim","text":"The Full IP Trim module of the GUI executes the K-Modulation on a selected Interaction Point with its predefined quadrupoles. Each bigger experiment like ATLAS (IP1), ALICE (IP2), CMS (IP5) and LHCb (IP8) is referred to by its corresponding number in the LHC complex. Choose an IP. By using this module, the quadrupole identifier and several other parameters are predefined to the corresponding Interaction Points. After the modulation is done, the trim data is present on the database and can be visualized and analyzed using the measurement view dialog. In order to start the K-Modulation on a magnet, the power converter has to be accessed using the CERN provided LSA Java API. The available parameters are the following: Delta I : Change of magnet strength in Amperes. Default value is 6.0A . Frequency : How fast the magnet strength is changed. Default value is 0.013 . Cycles : How many times the modulation is done. Default value is 3 . Select trim parameter.","title":"Performing a Full IP Trim"},{"location":"guis/kmod/full_ip_trim/#result-panel","text":"While the Full IP Trim is executed the current and the tune values are displayed live in the result panel. After the modulation is done, it will show the whole trim results in the corresponding ResultPanel of this trimming task. The trim results are stored in the save directory of the current IP . Trim result panel.","title":"Result Panel"},{"location":"guis/kmod/gui/","text":"The Kmod GUI \u00b6 The Kmod GUI provides functionality to perform K-Modulation either as a full IP trim or through an individual magnet circuit, in the form of a graphical interface. It also has the ability to extract and analyze data from previous trims. This site will guide you through the GUI's layout and functionality. For starters, check out the basics of running the GUI . Alternatively, a procedure for performing K-modulation in the CCC is described on this page .","title":"About"},{"location":"guis/kmod/gui/#the-kmod-gui","text":"The Kmod GUI provides functionality to perform K-Modulation either as a full IP trim or through an individual magnet circuit, in the form of a graphical interface. It also has the ability to extract and analyze data from previous trims. This site will guide you through the GUI's layout and functionality. For starters, check out the basics of running the GUI . Alternatively, a procedure for performing K-modulation in the CCC is described on this page .","title":"The Kmod GUI"},{"location":"guis/kmod/individual_modulation/","text":"Performing Individual Magnet Modulation \u00b6 The Kmod GUI also allows for a modulation of a given magnet circuit. This is used for example for the quadrupoles in the matching section of the experimental insertions, or in the RF insertion, where one circuits corresponds to one quadrupole. The Kmodulation data can then be used to extract the beta-functions at instruments (e.g. BSRT ) between the quadrupoles. As of 2018, the instruments for which the measured beta-function has been used for calibration are the Wirescanner ( BWS.5 ) and the undulator ( MU.B5 ) or dipole D3 ( MBRS.5 ) for the BSRT . All these instruments are located between Q5R and Q5L . Additionally, in beam 2, between MQM.7L4.B2 and MQY.6L4.B2 the BGV demonstrator is located ( B7L4.B2 ). Circuit Selection \u00b6 The first step is to select a magnet circuit to use. You can do so under Parameter Selection -> Select Quadrupole . The circuit corresponding to the quadrupole to be modulated has to be selected on the right hand side of the panel and added via the add quadrupole button to the left hand side. As example for the naming convention, the circuit RQ5/.R4B2/K1 corresponds to the MQY.5R4.B2 in the LHC sequence file. Under Trim Function , select Sine Function before continuing. Select circuits. Trim Start \u00b6 The trim current, frequency, and number of cycles should be entered on the right hand side of the window and need to be set via the apply settings button. Normal values for trim I in IR4 are 2A at injection for the Q5 and 12A at 6.5 TeV for the Q5 . Values for other magnets can be found in the elogbook in the shifts logs of the 28/04/2018 and 07/10/2016 . Data acquisition is started with the Start Acquiring button, following by starting the trim with the Start trim button. After the trim is finished, acquisition of data needs to be stopped using the Stop Acquiring button. Todo Include screenshot of trim start Warning The start and end time should be noted down in the elogbook for later data extraction, as no automatic extraction like in the IP modulation case exists. Trim Extraction \u00b6 After acquisition during a trim, data can then be extracted by selecting the circuit in the Select Quadrupole panel under Parameter selection and pushing the extract previous trim button. The trim start and end times as well as the beam energy need to be entered. In the following panel, the trim data can then be saved via the Save magnet measurement button. Todo Include screenshot of trim extraction The analysis of the extracted Kmod data is described in the next section .","title":"Magnet Modulation"},{"location":"guis/kmod/individual_modulation/#performing-individual-magnet-modulation","text":"The Kmod GUI also allows for a modulation of a given magnet circuit. This is used for example for the quadrupoles in the matching section of the experimental insertions, or in the RF insertion, where one circuits corresponds to one quadrupole. The Kmodulation data can then be used to extract the beta-functions at instruments (e.g. BSRT ) between the quadrupoles. As of 2018, the instruments for which the measured beta-function has been used for calibration are the Wirescanner ( BWS.5 ) and the undulator ( MU.B5 ) or dipole D3 ( MBRS.5 ) for the BSRT . All these instruments are located between Q5R and Q5L . Additionally, in beam 2, between MQM.7L4.B2 and MQY.6L4.B2 the BGV demonstrator is located ( B7L4.B2 ).","title":"Performing Individual Magnet Modulation"},{"location":"guis/kmod/individual_modulation/#circuit-selection","text":"The first step is to select a magnet circuit to use. You can do so under Parameter Selection -> Select Quadrupole . The circuit corresponding to the quadrupole to be modulated has to be selected on the right hand side of the panel and added via the add quadrupole button to the left hand side. As example for the naming convention, the circuit RQ5/.R4B2/K1 corresponds to the MQY.5R4.B2 in the LHC sequence file. Under Trim Function , select Sine Function before continuing. Select circuits.","title":"Circuit Selection"},{"location":"guis/kmod/individual_modulation/#trim-start","text":"The trim current, frequency, and number of cycles should be entered on the right hand side of the window and need to be set via the apply settings button. Normal values for trim I in IR4 are 2A at injection for the Q5 and 12A at 6.5 TeV for the Q5 . Values for other magnets can be found in the elogbook in the shifts logs of the 28/04/2018 and 07/10/2016 . Data acquisition is started with the Start Acquiring button, following by starting the trim with the Start trim button. After the trim is finished, acquisition of data needs to be stopped using the Stop Acquiring button. Todo Include screenshot of trim start Warning The start and end time should be noted down in the elogbook for later data extraction, as no automatic extraction like in the IP modulation case exists.","title":"Trim Start"},{"location":"guis/kmod/individual_modulation/#trim-extraction","text":"After acquisition during a trim, data can then be extracted by selecting the circuit in the Select Quadrupole panel under Parameter selection and pushing the extract previous trim button. The trim start and end times as well as the beam energy need to be entered. In the following panel, the trim data can then be saved via the Save magnet measurement button. Todo Include screenshot of trim extraction The analysis of the extracted Kmod data is described in the next section .","title":"Trim Extraction"},{"location":"guis/kmod/load_model/","text":"Loading a model from into Kmod GUI \u00b6 You can load the model created from the Beta-Beat GUI or Multiturn GUI for each beam by pressing the load model button. Select the desired model directory in the pop-up dialog and confirm by pressing the open button. If everything worked fine and the specified folder contains the needed twiss_elements file, the GUI will display the model's name. You can choose another model directory by pressing on the update model button which appears if a model is already selected. The model data is used for preset values in the GUI and the orbit-offset script needs the model for its calculations. Loading a Model Setting the Beta-Beat.src directory To change the setting of the used Beta-Beat.src directory, navigate to File -> Set Beta-Beat dir and select the location where the Beta-Beat.src directory to be used is located. The Kmod GUI will automatically take the proper Python scripts from this directory. This is automatically reset to the standard directory /afs/cern.ch/eng/sl/lintrack/Beta-Beat.src/ . The GUI will check if the Beta-Beat.src directory contains the ProgramVersions.properties file, which is needed for calling an external python script. You can change the standard directory by creating a user.properties file in the project folder. The kmod GUI will automatically read this file and change the Beta-Beat.src directory to this new value. This file must contain BetaBeatDir=[newBBdir] (replace the placeholder correctly).","title":"Loading a Model"},{"location":"guis/kmod/load_model/#loading-a-model-from-into-kmod-gui","text":"You can load the model created from the Beta-Beat GUI or Multiturn GUI for each beam by pressing the load model button. Select the desired model directory in the pop-up dialog and confirm by pressing the open button. If everything worked fine and the specified folder contains the needed twiss_elements file, the GUI will display the model's name. You can choose another model directory by pressing on the update model button which appears if a model is already selected. The model data is used for preset values in the GUI and the orbit-offset script needs the model for its calculations. Loading a Model Setting the Beta-Beat.src directory To change the setting of the used Beta-Beat.src directory, navigate to File -> Set Beta-Beat dir and select the location where the Beta-Beat.src directory to be used is located. The Kmod GUI will automatically take the proper Python scripts from this directory. This is automatically reset to the standard directory /afs/cern.ch/eng/sl/lintrack/Beta-Beat.src/ . The GUI will check if the Beta-Beat.src directory contains the ProgramVersions.properties file, which is needed for calling an external python script. You can change the standard directory by creating a user.properties file in the project folder. The kmod GUI will automatically read this file and change the Beta-Beat.src directory to this new value. This file must contain BetaBeatDir=[newBBdir] (replace the placeholder correctly).","title":"Loading a model from into Kmod GUI"},{"location":"guis/kmod/trim_analysis/","text":"Analyzing Trim Data \u00b6 The Kmod GUI provides functionality to load previously recorded trim data and perform a k-modulation analysis of the results. The steps are described below. Extract Trim Panel \u00b6 If you want to extract data from a previous trim, press on the Extract Previous Trim button. Input the right energy value for this trim in the top right input field and press the Select trim or the Last Trim button. Just ignore the rest (TODO: why???) The Select trim button will open the trim selection panel and the Last trim button will just load the last trim entry from Timber. The \"Select Trim\" Panel. If you chose Select trim , then select the desired IP then navigate othe desired date and confirm with pressing on the green Select trim button. You can switch between different IPs and dates by using the combo boxes. Select Trim by Date. Measurement View Dialog \u00b6 The selected/loaded trim will then be displayed in the Measurement View Dialog , from which you can analyze the trim data with different options. Measurement View. To get beta-star values, press the Analyze Trim button , then in the next dialog select the directory in which to save the trim data and the results. You will need to input the estimated beta star in the horizontal and vertical planes as well as the waist shift for each beam. The values are loaded from the model files. If no model is loaded, you have to input those values. The (old) python code of this script can be found here . The results are stored in the file named beta_star.out . Estimated beta-star input. Special Case: Individual Magnet Modulation If individual circuits/magnets have been modulated, the analysis cannot (yet) be triggered from the GUI. Instead, the gui2kmod.py script has to be called from the command line. For the input arguments, the working directory ( --working_directory ) should point to a folder where the extracted data of both modulated quadrupoles is present and the --circuit option should be selected, followed by the two circuit names. The initial guesses should be set to a large beta function and waist (e.g. --BetastarAndWaist 200,-100 ) to speed up the convergence. An example script call would look as follow: gui2kmod.py --circuit rq5.l4b1,rq5.r4b1 --beam b1 --BetastarAndWaist 200 ,-100 --instruments MONITOR,RBEND,INSTRUMENT,TKICKER --working_directory /path/to/data Results are then saved in the working directory in a folder with the magnet names. Todo I think Hector took care of Kmod analysis for omc3 so maybe the above tip section should point to the newest codes?","title":"Analyzing Trim Data"},{"location":"guis/kmod/trim_analysis/#analyzing-trim-data","text":"The Kmod GUI provides functionality to load previously recorded trim data and perform a k-modulation analysis of the results. The steps are described below.","title":"Analyzing Trim Data"},{"location":"guis/kmod/trim_analysis/#extract-trim-panel","text":"If you want to extract data from a previous trim, press on the Extract Previous Trim button. Input the right energy value for this trim in the top right input field and press the Select trim or the Last Trim button. Just ignore the rest (TODO: why???) The Select trim button will open the trim selection panel and the Last trim button will just load the last trim entry from Timber. The \"Select Trim\" Panel. If you chose Select trim , then select the desired IP then navigate othe desired date and confirm with pressing on the green Select trim button. You can switch between different IPs and dates by using the combo boxes. Select Trim by Date.","title":"Extract Trim Panel"},{"location":"guis/kmod/trim_analysis/#measurement-view-dialog","text":"The selected/loaded trim will then be displayed in the Measurement View Dialog , from which you can analyze the trim data with different options. Measurement View. To get beta-star values, press the Analyze Trim button , then in the next dialog select the directory in which to save the trim data and the results. You will need to input the estimated beta star in the horizontal and vertical planes as well as the waist shift for each beam. The values are loaded from the model files. If no model is loaded, you have to input those values. The (old) python code of this script can be found here . The results are stored in the file named beta_star.out . Estimated beta-star input. Special Case: Individual Magnet Modulation If individual circuits/magnets have been modulated, the analysis cannot (yet) be triggered from the GUI. Instead, the gui2kmod.py script has to be called from the command line. For the input arguments, the working directory ( --working_directory ) should point to a folder where the extracted data of both modulated quadrupoles is present and the --circuit option should be selected, followed by the two circuit names. The initial guesses should be set to a large beta function and waist (e.g. --BetastarAndWaist 200,-100 ) to speed up the convergence. An example script call would look as follow: gui2kmod.py --circuit rq5.l4b1,rq5.r4b1 --beam b1 --BetastarAndWaist 200 ,-100 --instruments MONITOR,RBEND,INSTRUMENT,TKICKER --working_directory /path/to/data Results are then saved in the working directory in a folder with the magnet names. Todo I think Hector took care of Kmod analysis for omc3 so maybe the above tip section should point to the newest codes?","title":"Measurement View Dialog"},{"location":"guis/multiturn/gui/","text":"The Multiturn GUI \u00b6","title":"About"},{"location":"guis/multiturn/gui/#the-multiturn-gui","text":"","title":"The Multiturn GUI"},{"location":"omc_team/about/","text":"The OMC Team \u00b6 Overview \u00b6 The OMC team is part of the ABP group at CERN , in the LNO section. We are responsible for the measurement and correction of the linear and non-linear accelerator optics in the LHC as well as other machines is the CERN accelerator complex. Our meetings can be found on our Indico page . Note - Documentation Status This site is currently under construction, and subject to constant change. A previous (but poor) documentation used to be the OMC twiki , and its relevant contents have been transfered here.","title":"About"},{"location":"omc_team/about/#the-omc-team","text":"","title":"The OMC Team"},{"location":"omc_team/about/#overview","text":"The OMC team is part of the ABP group at CERN , in the LNO section. We are responsible for the measurement and correction of the linear and non-linear accelerator optics in the LHC as well as other machines is the CERN accelerator complex. Our meetings can be found on our Indico page . Note - Documentation Status This site is currently under construction, and subject to constant change. A previous (but poor) documentation used to be the OMC twiki , and its relevant contents have been transfered here.","title":"Overview"},{"location":"omc_team/ideas/","text":"Future MD Ideas \u00b6 Run 3 MDs Ideas \u00b6 Linear Optics Related \u00b6 2 New ballistic optics flavors: Telescopic for tune jitter measurements with enhanced arcs. Ballistic with large vertical dispersion. Half integer tune. 60 degress arc cell phase advance for HE-LHC (or \u2153 HE-LHC). Experience HL-LHC-like injection optics with beta*=6m (pilots only). Look at 11 T flux jumps with TbT data, possibly using a new optics with enhanced beta functions. 2 possible experiments with TOTEM to understand the assymetry of the local dispersion in IR5: Measurements with flat orbit, >60 bunches. Measurements with ballistic optics. Non-90 degrees arc cell phase advance to ease correction of errors in the arcs (reduce the knob degeneracy of orbit bumps at sextupoles). Coupling Related \u00b6 Switching off MCS and main sextupoles in one arc to evaluate possible (systematic) vertical or horizontal misalignment of dipoles by measuring coupling and beta-beating during the b3 decay. In a previous MD there was a quadratic coupling increase due to the orbit bump in the arc (maybe the one without MS, tbc). Crazy MD for this, could be shifting the edge of some dipoles up to see change in coupling. Tilting a triplet quadrupole to check validity of the tilt measurement (with and/or without beam). BPM related \u00b6 Measure bad BPMs with low and high intensities to detect possible issues with the comparator (transistor). Also swap connections between good and bad BPM to identified. Have one BPM unplugged to detect its noise until next technical stop. Momentum compaction factor from Qs versus Voltage. Snap-back with forced 3D oscillations (optics, chroma,...). Maybe not OMC: 24h pilot at top energy to use damped beam for BSRT resolution measurement. 0.5 um bunch from new injector chain?? Good for optics measurements, BSRT calibration etc. K-mod related \u00b6 K-Modulation with Xing angle and OFB off for Leon's Xing angle reconstruction. Arc K-modulation. K-modulation in IR4 during the energy ramp. Reduce to one the DCCTs per circuit with pilot in the machine and ATS optics to see the impact of 2 DCCTs. Michele Martino also proposes increasing in 1ppm the noise of the DCCTs in one or all dipole PCs to see impact on tune jitter. Non-Linear Optics Related \u00b6 Commissioning?: Measure amplitude detuning coming from CMS solenoid by having the on and off measurements. IR non-linear corrector offsets measurement. Emittance growth from octupoles and chroma. Amplitude detuning measurement versus crossing angle with b6 corrector on, for 5 crossing angles to evaluate performance of the technique for b6 correction. Amplitude dependent optics: Dedicated settings at injection as PoP (Thomas & Barbara), alternative techniques in parallel (K-modulation with decohered beams and amplitude dependent segment-by-segment). IR2 non-linear correction -> Dedicated squeeze for IR2 to allow for good triplet characterizaton to shed light on b4. Amplitude detuning in ballistic optics. ADT for single kicks. b5 RDTs. Measurement and correction of a3 RDTs at injection for lifetime optimization (correcting with MSSX and MSS). Reduce RF voltage to decrease synchrotron tune to limit the emittance blow-up during excitation with driven and natural tunes close to each other.","title":"Future MD Ideas"},{"location":"omc_team/ideas/#future-md-ideas","text":"","title":"Future MD Ideas"},{"location":"omc_team/ideas/#run-3-mds-ideas","text":"","title":"Run 3 MDs Ideas"},{"location":"omc_team/ideas/#linear-optics-related","text":"2 New ballistic optics flavors: Telescopic for tune jitter measurements with enhanced arcs. Ballistic with large vertical dispersion. Half integer tune. 60 degress arc cell phase advance for HE-LHC (or \u2153 HE-LHC). Experience HL-LHC-like injection optics with beta*=6m (pilots only). Look at 11 T flux jumps with TbT data, possibly using a new optics with enhanced beta functions. 2 possible experiments with TOTEM to understand the assymetry of the local dispersion in IR5: Measurements with flat orbit, >60 bunches. Measurements with ballistic optics. Non-90 degrees arc cell phase advance to ease correction of errors in the arcs (reduce the knob degeneracy of orbit bumps at sextupoles).","title":"Linear Optics Related"},{"location":"omc_team/ideas/#coupling-related","text":"Switching off MCS and main sextupoles in one arc to evaluate possible (systematic) vertical or horizontal misalignment of dipoles by measuring coupling and beta-beating during the b3 decay. In a previous MD there was a quadratic coupling increase due to the orbit bump in the arc (maybe the one without MS, tbc). Crazy MD for this, could be shifting the edge of some dipoles up to see change in coupling. Tilting a triplet quadrupole to check validity of the tilt measurement (with and/or without beam).","title":"Coupling Related"},{"location":"omc_team/ideas/#bpm-related","text":"Measure bad BPMs with low and high intensities to detect possible issues with the comparator (transistor). Also swap connections between good and bad BPM to identified. Have one BPM unplugged to detect its noise until next technical stop. Momentum compaction factor from Qs versus Voltage. Snap-back with forced 3D oscillations (optics, chroma,...). Maybe not OMC: 24h pilot at top energy to use damped beam for BSRT resolution measurement. 0.5 um bunch from new injector chain?? Good for optics measurements, BSRT calibration etc.","title":"BPM related"},{"location":"omc_team/ideas/#k-mod-related","text":"K-Modulation with Xing angle and OFB off for Leon's Xing angle reconstruction. Arc K-modulation. K-modulation in IR4 during the energy ramp. Reduce to one the DCCTs per circuit with pilot in the machine and ATS optics to see the impact of 2 DCCTs. Michele Martino also proposes increasing in 1ppm the noise of the DCCTs in one or all dipole PCs to see impact on tune jitter.","title":"K-mod related"},{"location":"omc_team/ideas/#non-linear-optics-related","text":"Commissioning?: Measure amplitude detuning coming from CMS solenoid by having the on and off measurements. IR non-linear corrector offsets measurement. Emittance growth from octupoles and chroma. Amplitude detuning measurement versus crossing angle with b6 corrector on, for 5 crossing angles to evaluate performance of the technique for b6 correction. Amplitude dependent optics: Dedicated settings at injection as PoP (Thomas & Barbara), alternative techniques in parallel (K-modulation with decohered beams and amplitude dependent segment-by-segment). IR2 non-linear correction -> Dedicated squeeze for IR2 to allow for good triplet characterizaton to shed light on b4. Amplitude detuning in ballistic optics. ADT for single kicks. b5 RDTs. Measurement and correction of a3 RDTs at injection for lifetime optimization (correcting with MSSX and MSS). Reduce RF voltage to decrease synchrotron tune to limit the emittance blow-up during excitation with driven and natural tunes close to each other.","title":"Non-Linear Optics Related"},{"location":"omc_team/methods/","text":"OMC Methods \u00b6 Note Very long content about the physics and practice of OMC methods + paper references for each (K-mod, 3D kicks, N-BPM...). Each method should be a H2, see examples below. N-BPM \u00b6 K-Modulation \u00b6 3D Kicks \u00b6","title":"OMC Methods"},{"location":"omc_team/methods/#omc-methods","text":"Note Very long content about the physics and practice of OMC methods + paper references for each (K-mod, 3D kicks, N-BPM...). Each method should be a H2, see examples below.","title":"OMC Methods"},{"location":"omc_team/methods/#n-bpm","text":"","title":"N-BPM"},{"location":"omc_team/methods/#k-modulation","text":"","title":"K-Modulation"},{"location":"omc_team/methods/#3d-kicks","text":"","title":"3D Kicks"},{"location":"omc_team/physics/","text":"The Pysics of OMC \u00b6 Note Long and detailed note of the kind of accelerator physcis that OMC takes care of. Includes the prerequisites of accelerator physics necessary to do OMC work / understand OMC methods. See example headers below. Basics of Accelerator Physics \u00b6 LHC IR Linear Optics \u00b6 LHC IR NonLinear Optics \u00b6","title":"Physics of OMC"},{"location":"omc_team/physics/#the-pysics-of-omc","text":"Note Long and detailed note of the kind of accelerator physcis that OMC takes care of. Includes the prerequisites of accelerator physics necessary to do OMC work / understand OMC methods. See example headers below.","title":"The Pysics of OMC"},{"location":"omc_team/physics/#basics-of-accelerator-physics","text":"","title":"Basics of Accelerator Physics"},{"location":"omc_team/physics/#lhc-ir-linear-optics","text":"","title":"LHC IR Linear Optics"},{"location":"omc_team/physics/#lhc-ir-nonlinear-optics","text":"","title":"LHC IR NonLinear Optics"},{"location":"omc_team/publications/","text":"OMC Publications \u00b6 Journal Publications \u00b6 2020 \u00b6 Detection of faulty beam position monitors using unsupervised learning, E. Fol, R. Tom\u00e1s, J. Coello de Portugal, and G. Franchetti Phys. Rev. Accel. Beams 23, 102805, 2020 @article{faulty_bpm_detection, title = {Detection of faulty beam position monitors using unsupervised learning}, author = {Fol, E. and Tom\\'as, R. and Coello de Portugal, J. and Franchetti, G.}, journal = {Phys. Rev. Accel. Beams}, volume = {23}, issue = {10}, pages = {102805}, numpages = {10}, year = {2020}, month = {Oct}, publisher = {American Physical Society}, doi = {10.1103/PhysRevAccelBeams.23.102805}, url = {https://link.aps.org/doi/10.1103/PhysRevAccelBeams.23.102805} } 2019 \u00b6 2018 \u00b6 Strategy Reports \u00b6 MD Notes \u00b6 2020 \u00b6 2019 \u00b6 2018 \u00b6","title":"Publications"},{"location":"omc_team/publications/#omc-publications","text":"","title":"OMC Publications"},{"location":"omc_team/publications/#journal-publications","text":"","title":"Journal Publications"},{"location":"omc_team/publications/#2020","text":"Detection of faulty beam position monitors using unsupervised learning, E. Fol, R. Tom\u00e1s, J. Coello de Portugal, and G. Franchetti Phys. Rev. Accel. Beams 23, 102805, 2020 @article{faulty_bpm_detection, title = {Detection of faulty beam position monitors using unsupervised learning}, author = {Fol, E. and Tom\\'as, R. and Coello de Portugal, J. and Franchetti, G.}, journal = {Phys. Rev. Accel. Beams}, volume = {23}, issue = {10}, pages = {102805}, numpages = {10}, year = {2020}, month = {Oct}, publisher = {American Physical Society}, doi = {10.1103/PhysRevAccelBeams.23.102805}, url = {https://link.aps.org/doi/10.1103/PhysRevAccelBeams.23.102805} }","title":"2020"},{"location":"omc_team/publications/#2019","text":"","title":"2019"},{"location":"omc_team/publications/#2018","text":"","title":"2018"},{"location":"omc_team/publications/#strategy-reports","text":"","title":"Strategy Reports"},{"location":"omc_team/publications/#md-notes","text":"","title":"MD Notes"},{"location":"omc_team/publications/#2020_1","text":"","title":"2020"},{"location":"omc_team/publications/#2019_1","text":"","title":"2019"},{"location":"omc_team/publications/#2018_1","text":"","title":"2018"},{"location":"packages/about/","text":"OMC's Python Packages \u00b6 This section of the website goes over our computing and analysis softwares, which are written in Python. Our main analysis software exist in two versions: Beta-Beat.src for Python 2, and its replacement omc3 for Python 3. Development of the Beta-Beat.src package is limited to bug fixes, and new features are integrated directly into omc3 . The pylhc package provides useful tools and scripts for our day-to-day work, and our other packages provide specific I/O and entrypoint utilities for our codes. All our Python codes, including legacy repositories, can be found on the PyLHC organisation 's page on Github. The OMC team develops and maintains the following packages: Quick Access Links \u00b6 OMC3 (Python 3.7+): frequency analysis, optics computation from turn-by-turn data, corrections calculations and results plotting. PyLHC-Tools (Python 3.7+): useful OMC-related scripts. Example Study Scripts (MAD-X): collection of example studies. Beta-Beat.src (Python 2.7): frequency analysis, optics computation from turn-by-turn data and corrections calculations. TFS-Pandas (Python 3.6+): TFS files I/O functionality. SDDS-Reader (Python 3.6+): SDDS files I/O functionality. Generic-Parser (Python 3.6+): entrypoint argument parser functionality.","title":"OMC Packages"},{"location":"packages/about/#omcs-python-packages","text":"This section of the website goes over our computing and analysis softwares, which are written in Python. Our main analysis software exist in two versions: Beta-Beat.src for Python 2, and its replacement omc3 for Python 3. Development of the Beta-Beat.src package is limited to bug fixes, and new features are integrated directly into omc3 . The pylhc package provides useful tools and scripts for our day-to-day work, and our other packages provide specific I/O and entrypoint utilities for our codes. All our Python codes, including legacy repositories, can be found on the PyLHC organisation 's page on Github. The OMC team develops and maintains the following packages:","title":"OMC's Python Packages"},{"location":"packages/about/#quick-access-links","text":"OMC3 (Python 3.7+): frequency analysis, optics computation from turn-by-turn data, corrections calculations and results plotting. PyLHC-Tools (Python 3.7+): useful OMC-related scripts. Example Study Scripts (MAD-X): collection of example studies. Beta-Beat.src (Python 2.7): frequency analysis, optics computation from turn-by-turn data and corrections calculations. TFS-Pandas (Python 3.6+): TFS files I/O functionality. SDDS-Reader (Python 3.6+): SDDS files I/O functionality. Generic-Parser (Python 3.6+): entrypoint argument parser functionality.","title":"Quick Access Links"},{"location":"packages/development/contributing/","text":"Contributing to the PyLHC Packages \u00b6 Want to report a bug, request a feature or simply contribute some code? We welcome contributions, but before you do, please read the following guidelines. Submission context \u00b6 Got a question or problem? \u00b6 If you have questions on some of the packages' functionality, and the available documentation does not provide answers, you can submit them as new issues on GitHub. If you spot missing parts in the documentation, feel free to report it in an issue and open a Pull Request that fixes it. Found a bug? \u00b6 If you found a bug in the source code, you can help us by submitting a bug report in a new issue on GitHub. If you wish to contribute a solution, you can submit a Pull Request with a fix. However, before doing so, please read the submission guidelines bellow. Missing a feature? \u00b6 You can request a new feature by opening an issue on GitHub. If you would like to implement a new feature, please submit an issue with a proposal first, to be sure that it is necessary and appropriate to the package, and to discuss implementation details. This will also allow us to better coordinate our efforts, prevent duplication of work, and help you craft the change so that it is successfully accepted into the project. Our Availability Please note we are busy people and developping these packages is not our primary priority at work. We may take some time to reply. Submission guidelines \u00b6 Submitting an issue \u00b6 Before you submit an issue, please search the issue tracker, maybe an issue for your problem already exists and the discussion might inform you of fixes or workarounds readily available. If you are submitting a bug report, please also provide a minimal scenario to reproduce it. Submitting a Pull Request (PR) \u00b6 First, search GitHub for an open or closed PR that relates to your submission. If you do not find a related issue or PR, or if your PR is the implementation for an issue you open, go ahead. Development : Clone the project, set up your branch and development environment, make your changes, and add descriptive messages to your commits. Please reference the issue number in your commit header messages so that your commits appear on the issue tracker. Build : Before submitting changes, please make sure tests pass and that the package properly installs. Most projects come with a Makefile to help with this, and you can get an overview of the available targets with make help . Pull Request : After having worked on your changes and pushed them to Github, open a Pull Request to the master branch. Review and approval by at least one of our team members is required before accepting changes. If new changes are suggested, make the required updates and push the changes again. Please do not require a review until all the quality checks pass. Quality checks \u00b6 Unit and accuracy tests are run automatically through CI Github Actions . A README.md file in the .github/workflows directory details our CI jobs. Additional checks for code-complexity, design-rules, test-coverage and duplication are made through CodeClimate . Pull requests implementing functionality or fixes are merged into the master branch after passing CI, and a reviewer's approval. After your PR is accepted by a team member, please select squash and merge to merge into master. Afterwards, you can safely delete your branch and close the issue. Python Guidelines \u00b6 We strive to use a common codestyle for our software. Please follow these guidelines to keep code cohesion up, and git diffs down. In case you want to contribute to a package's development, you should install it in editable mode: git clone https://github.com/pylhc/package_name pip install --editable package_name Installing Extras You can install extra dependencies (as defined in setup.py ) suited to your use case with the following commands: pip install --editable package_name[test] pip install --editable package_name[test,doc] pip install --editable package_name[all] For development purposes, we recommend using the all extra to be fully set up. Naming Conventions \u00b6 Overall, please abide by PEP8 : Module, functions, methods, attributes and local variable names: use snake_case . Example: segment_by_segment.py , get_phase.py . Class names: use PascalCase . Example: FileWriter , FixedTfs . Module constants: Uppercase words divided by underscores. Example: INDEX_ID , DEFAULT_COLUMN_WIDTH . Private methods, functions and variables: see above, but precede the name by an underscore. Example: _validate_index_type , _get_header_line . Code Structure \u00b6 Make the code as readable as possible, both for collaborators and future you. Use descriptive variable names. Use descriptive function names. Type hint your code. Respect the max line length of 100 characters as much as possible. Divide code blocks into simple functions. Strive to write pure functions . Avoid code duplication. Respect the Zen of Python . We use PyCharm as IDE in the team. Docstrings Convention \u00b6 To be compatible with our automatic API documentation generator, please respect Sphinx conventions.","title":"Contributing"},{"location":"packages/development/contributing/#contributing-to-the-pylhc-packages","text":"Want to report a bug, request a feature or simply contribute some code? We welcome contributions, but before you do, please read the following guidelines.","title":"Contributing to the PyLHC Packages"},{"location":"packages/development/contributing/#submission-context","text":"","title":"Submission context"},{"location":"packages/development/contributing/#got-a-question-or-problem","text":"If you have questions on some of the packages' functionality, and the available documentation does not provide answers, you can submit them as new issues on GitHub. If you spot missing parts in the documentation, feel free to report it in an issue and open a Pull Request that fixes it.","title":"Got a question or problem?"},{"location":"packages/development/contributing/#found-a-bug","text":"If you found a bug in the source code, you can help us by submitting a bug report in a new issue on GitHub. If you wish to contribute a solution, you can submit a Pull Request with a fix. However, before doing so, please read the submission guidelines bellow.","title":"Found a bug?"},{"location":"packages/development/contributing/#missing-a-feature","text":"You can request a new feature by opening an issue on GitHub. If you would like to implement a new feature, please submit an issue with a proposal first, to be sure that it is necessary and appropriate to the package, and to discuss implementation details. This will also allow us to better coordinate our efforts, prevent duplication of work, and help you craft the change so that it is successfully accepted into the project. Our Availability Please note we are busy people and developping these packages is not our primary priority at work. We may take some time to reply.","title":"Missing a feature?"},{"location":"packages/development/contributing/#submission-guidelines","text":"","title":"Submission guidelines"},{"location":"packages/development/contributing/#submitting-an-issue","text":"Before you submit an issue, please search the issue tracker, maybe an issue for your problem already exists and the discussion might inform you of fixes or workarounds readily available. If you are submitting a bug report, please also provide a minimal scenario to reproduce it.","title":"Submitting an issue"},{"location":"packages/development/contributing/#submitting-a-pull-request-pr","text":"First, search GitHub for an open or closed PR that relates to your submission. If you do not find a related issue or PR, or if your PR is the implementation for an issue you open, go ahead. Development : Clone the project, set up your branch and development environment, make your changes, and add descriptive messages to your commits. Please reference the issue number in your commit header messages so that your commits appear on the issue tracker. Build : Before submitting changes, please make sure tests pass and that the package properly installs. Most projects come with a Makefile to help with this, and you can get an overview of the available targets with make help . Pull Request : After having worked on your changes and pushed them to Github, open a Pull Request to the master branch. Review and approval by at least one of our team members is required before accepting changes. If new changes are suggested, make the required updates and push the changes again. Please do not require a review until all the quality checks pass.","title":"Submitting a Pull Request (PR)"},{"location":"packages/development/contributing/#quality-checks","text":"Unit and accuracy tests are run automatically through CI Github Actions . A README.md file in the .github/workflows directory details our CI jobs. Additional checks for code-complexity, design-rules, test-coverage and duplication are made through CodeClimate . Pull requests implementing functionality or fixes are merged into the master branch after passing CI, and a reviewer's approval. After your PR is accepted by a team member, please select squash and merge to merge into master. Afterwards, you can safely delete your branch and close the issue.","title":"Quality checks"},{"location":"packages/development/contributing/#python-guidelines","text":"We strive to use a common codestyle for our software. Please follow these guidelines to keep code cohesion up, and git diffs down. In case you want to contribute to a package's development, you should install it in editable mode: git clone https://github.com/pylhc/package_name pip install --editable package_name Installing Extras You can install extra dependencies (as defined in setup.py ) suited to your use case with the following commands: pip install --editable package_name[test] pip install --editable package_name[test,doc] pip install --editable package_name[all] For development purposes, we recommend using the all extra to be fully set up.","title":"Python Guidelines"},{"location":"packages/development/contributing/#naming-conventions","text":"Overall, please abide by PEP8 : Module, functions, methods, attributes and local variable names: use snake_case . Example: segment_by_segment.py , get_phase.py . Class names: use PascalCase . Example: FileWriter , FixedTfs . Module constants: Uppercase words divided by underscores. Example: INDEX_ID , DEFAULT_COLUMN_WIDTH . Private methods, functions and variables: see above, but precede the name by an underscore. Example: _validate_index_type , _get_header_line .","title":"Naming Conventions"},{"location":"packages/development/contributing/#code-structure","text":"Make the code as readable as possible, both for collaborators and future you. Use descriptive variable names. Use descriptive function names. Type hint your code. Respect the max line length of 100 characters as much as possible. Divide code blocks into simple functions. Strive to write pure functions . Avoid code duplication. Respect the Zen of Python . We use PyCharm as IDE in the team.","title":"Code Structure"},{"location":"packages/development/contributing/#docstrings-convention","text":"To be compatible with our automatic API documentation generator, please respect Sphinx conventions.","title":"Docstrings Convention"},{"location":"packages/development/howto_release/","text":"How to make a release of a python package \u00b6 Todo Detailed description. Remove GIF.","title":"Releases"},{"location":"packages/development/howto_release/#how-to-make-a-release-of-a-python-package","text":"Todo Detailed description. Remove GIF.","title":"How to make a release of a python package"},{"location":"packages/development/howto_venv/","text":"How to Create Python Virtual Environments \u00b6 Both for the development of Python codes as well as running some of the existing codes, the creation of a virtual Python environment is recommended. This not only prevents conflicts when installing multiple packages with different required versions of the same package, but also allows to separate production and test scenarios. Python Virtual Environments Don't know what a virtual environment is or the difference between these many tools? Here is a good (but lengthy) primer on virtual environments by RealPython. Python environments are created using the built-in venv module, with the old pyvenv being deprecated since Python 3.6 and as such not recommended. The following description is a summary of the steps presented here and here , targetting Unix users. What About Anaconda? Note that the Anaconda Python distribution provides the conda tool as a package manager, which also allows the creation of Python virtual environments. However, due to the simplicity of the venv module, only this module will be described in the following. Virtual Environments with pyvenv \u00b6 To create a virtual environment in the folder testpython , run the command python -m venv ./testpython This creates a python environment in the testpython folder, linking to the original Python installation. As such, all packages of the original installation are available. However, using a clean and up-to-date Python 3 installation is recommended. Python Installations Multiple installations are available in the GPN . An installation maintained by the OMC-team can be found at /afs/cern.ch/eng/sl/lintrack/anaconda3/bin . The general Python distribution acc-py , maintained by BE/CO, is detailed in this wiki . To activate the created python installation, run source ./testpython/env/activate Now, the alias python should link to the installation in the testpython folder instead of the standard installation. This can be checked on Unix systems with the command which python . Note that the following command also provides access to the new installation and can be used to run scripts: ./testpython/bin/python To leave the Python environment, run: deactivate After the activation of the environment, packages can be installed with pip . The packages installed this way will be put in the testpython folder structure and will thus not affect the main system installation. Packages installed here will have priority over the packages of the original distribution when you have activated the environment. Different ways to use pip for package installations are nicely explained in the official documentation . Example use case \u00b6 In the following, a typical example is shown how to create a virtual environment in the current working directory and install omc3 to perform an optics analysis. First, create and activate a virtual environment: /afs/cern.ch/eng/sl/lintrack/anaconda3/bin/python -m venv ./omcpython source ./omcpython/env/activate Then, clone and install the omc3 package (since it is not yet deployed on PyPI): git clone https://github.com/pylhc/omc3 pip install --editable ./omc3 [ all ] Finally, run your analysis: python -m omc3.hole_in_one --harpy --files ... Note when installing omc3 , required packages such as tfs-pandas are automatically fetched and installed from PyPi . Furthermore, additional required packages for running the unittests, such as pytest , are also installed when specifying the all or test extras. To run the tests, run: python -m pytest ./omc3/tests","title":"Virtual Environments"},{"location":"packages/development/howto_venv/#how-to-create-python-virtual-environments","text":"Both for the development of Python codes as well as running some of the existing codes, the creation of a virtual Python environment is recommended. This not only prevents conflicts when installing multiple packages with different required versions of the same package, but also allows to separate production and test scenarios. Python Virtual Environments Don't know what a virtual environment is or the difference between these many tools? Here is a good (but lengthy) primer on virtual environments by RealPython. Python environments are created using the built-in venv module, with the old pyvenv being deprecated since Python 3.6 and as such not recommended. The following description is a summary of the steps presented here and here , targetting Unix users. What About Anaconda? Note that the Anaconda Python distribution provides the conda tool as a package manager, which also allows the creation of Python virtual environments. However, due to the simplicity of the venv module, only this module will be described in the following.","title":"How to Create Python Virtual Environments"},{"location":"packages/development/howto_venv/#virtual-environments-with-pyvenv","text":"To create a virtual environment in the folder testpython , run the command python -m venv ./testpython This creates a python environment in the testpython folder, linking to the original Python installation. As such, all packages of the original installation are available. However, using a clean and up-to-date Python 3 installation is recommended. Python Installations Multiple installations are available in the GPN . An installation maintained by the OMC-team can be found at /afs/cern.ch/eng/sl/lintrack/anaconda3/bin . The general Python distribution acc-py , maintained by BE/CO, is detailed in this wiki . To activate the created python installation, run source ./testpython/env/activate Now, the alias python should link to the installation in the testpython folder instead of the standard installation. This can be checked on Unix systems with the command which python . Note that the following command also provides access to the new installation and can be used to run scripts: ./testpython/bin/python To leave the Python environment, run: deactivate After the activation of the environment, packages can be installed with pip . The packages installed this way will be put in the testpython folder structure and will thus not affect the main system installation. Packages installed here will have priority over the packages of the original distribution when you have activated the environment. Different ways to use pip for package installations are nicely explained in the official documentation .","title":"Virtual Environments with pyvenv"},{"location":"packages/development/howto_venv/#example-use-case","text":"In the following, a typical example is shown how to create a virtual environment in the current working directory and install omc3 to perform an optics analysis. First, create and activate a virtual environment: /afs/cern.ch/eng/sl/lintrack/anaconda3/bin/python -m venv ./omcpython source ./omcpython/env/activate Then, clone and install the omc3 package (since it is not yet deployed on PyPI): git clone https://github.com/pylhc/omc3 pip install --editable ./omc3 [ all ] Finally, run your analysis: python -m omc3.hole_in_one --harpy --files ... Note when installing omc3 , required packages such as tfs-pandas are automatically fetched and installed from PyPi . Furthermore, additional required packages for running the unittests, such as pytest , are also installed when specifying the all or test extras. To run the tests, run: python -m pytest ./omc3/tests","title":"Example use case"},{"location":"packages/mess/about/","text":"MESS - MAD-X Example Study Scripts \u00b6 This repository is a collection of MAD-X scripts used for various studies in the optics measurements and corrections group (OMC). Todo Examples of are missing. Maybe just copy-paste of the README.md files in the repository. Getting Started \u00b6 The scripts can be browsed via github or the full repository can be obtained either via git clone https://github.com/pylhc/MESS.git or downloading the zipped repository. Prerequisites \u00b6 To run the scripts, MAD-X is required. If not otherwise stated, all scripts have been tested using MAD-X > 5.05.02. Documentation \u00b6 Each script directory contains a README , outlining the basic functionality and notes on possible pitfalls. Excessive use of comments in the MAD-X scripts itself is encouraged. Maintainability \u00b6 The main scripts should be named job.madx and placed in an accordingly named directory in the directory tree. Supporting files should be uploaded in the script directory. Links to external afs directories should be avoided as files might be modified there or removed. Running with the minimum amount of unavoidable MAD-X errors is prefered. Studies \u00b6 LHC - The flagship collider of the 21 st century Coupling RDT Bump - Creates closed coupling bumps in the LHC IR2 and Arc12. Sextupole RDT Bump - Creates closed sextupole RDT bump in Arc12. Injectionenergy with misaligments and correction - Realistic model of the LHC at injection energy with misaligments and nonlinear correction. Kmod simulation - Simulating K-Modulation in one Q1 quadrupole. Tracking with ACD - Setup for AC-dipole in LHC and subsequent tracking FODO Testlattice - Small FODO lattice for benchmarking theories and scripts Lattice Setup - Setting up basic lattice and return twiss. Phase Trombone - Setting up basic lattice, match tunes via a phase trombone and return twiss. PETRA3 - PETRA III, DESY's brilliant X-ray light source Model Creation - Creates model twiss files with AC-dipole and tune selection. Authors \u00b6 pyLHC/OMC-Team - Working Group - pyLHC License \u00b6 This project is licensed under the MIT License - see the LICENSE file for details","title":"About"},{"location":"packages/mess/about/#mess-mad-x-example-study-scripts","text":"This repository is a collection of MAD-X scripts used for various studies in the optics measurements and corrections group (OMC). Todo Examples of are missing. Maybe just copy-paste of the README.md files in the repository.","title":"MESS - MAD-X Example Study Scripts"},{"location":"packages/mess/about/#getting-started","text":"The scripts can be browsed via github or the full repository can be obtained either via git clone https://github.com/pylhc/MESS.git or downloading the zipped repository.","title":"Getting Started"},{"location":"packages/mess/about/#prerequisites","text":"To run the scripts, MAD-X is required. If not otherwise stated, all scripts have been tested using MAD-X > 5.05.02.","title":"Prerequisites"},{"location":"packages/mess/about/#documentation","text":"Each script directory contains a README , outlining the basic functionality and notes on possible pitfalls. Excessive use of comments in the MAD-X scripts itself is encouraged.","title":"Documentation"},{"location":"packages/mess/about/#maintainability","text":"The main scripts should be named job.madx and placed in an accordingly named directory in the directory tree. Supporting files should be uploaded in the script directory. Links to external afs directories should be avoided as files might be modified there or removed. Running with the minimum amount of unavoidable MAD-X errors is prefered.","title":"Maintainability"},{"location":"packages/mess/about/#studies","text":"LHC - The flagship collider of the 21 st century Coupling RDT Bump - Creates closed coupling bumps in the LHC IR2 and Arc12. Sextupole RDT Bump - Creates closed sextupole RDT bump in Arc12. Injectionenergy with misaligments and correction - Realistic model of the LHC at injection energy with misaligments and nonlinear correction. Kmod simulation - Simulating K-Modulation in one Q1 quadrupole. Tracking with ACD - Setup for AC-dipole in LHC and subsequent tracking FODO Testlattice - Small FODO lattice for benchmarking theories and scripts Lattice Setup - Setting up basic lattice and return twiss. Phase Trombone - Setting up basic lattice, match tunes via a phase trombone and return twiss. PETRA3 - PETRA III, DESY's brilliant X-ray light source Model Creation - Creates model twiss files with AC-dipole and tune selection.","title":"Studies"},{"location":"packages/mess/about/#authors","text":"pyLHC/OMC-Team - Working Group - pyLHC","title":"Authors"},{"location":"packages/mess/about/#license","text":"This project is licensed under the MIT License - see the LICENSE file for details","title":"License"},{"location":"packages/omc3/about/","text":"The OMC3 Software Documentation \u00b6 The omc3 package is the python tool of the Optics Measurements and Corrections team at CERN. The omc3 repository is the new version of our codes, refactored and rewritten for Python 3.7+ . This section acts as a general documentation and guide to using the omc3 package. The package's source can be found on Github and its API documentation can be found at the following link . What to expect \u00b6 The omc3 package serves the following purposes: Providing an all-in-one package for frequency analysis and optics measurements and corrections algorithms in particle accelerators. Providing an easily callable entrypoint to run your analytics from measurement / simulation files. Providing a convenient wrapper to effortlessly run MAD-X jobs. For detailed instructions see the getting started guide . Changelog \u00b6 A changelog file is made available in the Github repository, at the following link . License \u00b6 This software is licensed under the GNU GPLv3 License, see License . Authors \u00b6 This work is the result of combined efforts by members of the pylhc/omc-team working group. Contributions are welcome, but tightly controlled, see the guidelines page.","title":"About omc3"},{"location":"packages/omc3/about/#the-omc3-software-documentation","text":"The omc3 package is the python tool of the Optics Measurements and Corrections team at CERN. The omc3 repository is the new version of our codes, refactored and rewritten for Python 3.7+ . This section acts as a general documentation and guide to using the omc3 package. The package's source can be found on Github and its API documentation can be found at the following link .","title":"The OMC3 Software Documentation"},{"location":"packages/omc3/about/#what-to-expect","text":"The omc3 package serves the following purposes: Providing an all-in-one package for frequency analysis and optics measurements and corrections algorithms in particle accelerators. Providing an easily callable entrypoint to run your analytics from measurement / simulation files. Providing a convenient wrapper to effortlessly run MAD-X jobs. For detailed instructions see the getting started guide .","title":"What to expect"},{"location":"packages/omc3/about/#changelog","text":"A changelog file is made available in the Github repository, at the following link .","title":"Changelog"},{"location":"packages/omc3/about/#license","text":"This software is licensed under the GNU GPLv3 License, see License .","title":"License"},{"location":"packages/omc3/about/#authors","text":"This work is the result of combined efforts by members of the pylhc/omc-team working group. Contributions are welcome, but tightly controlled, see the guidelines page.","title":"Authors"},{"location":"packages/omc3/getting_started/","text":"Getting Started with OMC3 \u00b6 Quick start \u00b6 The omc3 package is Python 3.7+ compatible, but not yet deployed to PyPI. The best way to install is though pip and VCS: git clone https://github.com/pylhc/omc3 pip install /path/to/omc3 Or simply from the online master branch, which is stable: pip install git+https://github.com/pylhc/omc3.git After installing, codes can be run with either python -m omc3.SCRIPT --FLAG ARGUMENT or calling path to the .py file directly. Functionality \u00b6 Main Scripts \u00b6 Main scripts to be executed lie in the /omc3 module directly. These include: hole_in_one.py to perform frequency analysis on turn by turn BPM data and infer optics (and more) for a given accelerator. madx_wrapper.py to start a MAD-X run with a file or string as input. model_creator.py to provide optics models required for optics analysis. run_kmod.py to analyse data from K-modulation and return the measured optics functions. tbt_converter.py to convert different turn by turn datatypes to sdds, and add noise. amplitude_detuning_analysis.py to perform amp. det. analysis on optics data with tune correction. Plotting Scripts \u00b6 Plotting scripts for analysis outputs can be found in the /omc3/plotting submodule: plot_spectrum.py to generate plots from files generated by frequency analysis. plot_bbq.py to generate plots from files generated by BBQ analysis. plot_amplitude_detuning.py to generate plots from files generated by amplitude detuning analysis. plot_optics_measurements.py to generate plots from files generated by optics_measurements. plot_tfs.py all purpose tfs-file plotter. Other Scripts \u00b6 Other general utility scripts are in /omc3/scripts module: update_nattune_in_linfile.py to update the natural tune columns in the lin files by finding the highest peak in the spectrum in a given interval. write_madx_macros.py to generate MAD-X tracking macros with observation points from a twiss file. merge_kmod_results.py to merge lsa_results files created by kmod, and add the luminosity imbalance if the 4 needed IP/Beam files combination are present. A typical analysis workflow with omc3 is described in the next page .","title":"Getting Started"},{"location":"packages/omc3/getting_started/#getting-started-with-omc3","text":"","title":"Getting Started with OMC3"},{"location":"packages/omc3/getting_started/#quick-start","text":"The omc3 package is Python 3.7+ compatible, but not yet deployed to PyPI. The best way to install is though pip and VCS: git clone https://github.com/pylhc/omc3 pip install /path/to/omc3 Or simply from the online master branch, which is stable: pip install git+https://github.com/pylhc/omc3.git After installing, codes can be run with either python -m omc3.SCRIPT --FLAG ARGUMENT or calling path to the .py file directly.","title":"Quick start"},{"location":"packages/omc3/getting_started/#functionality","text":"","title":"Functionality"},{"location":"packages/omc3/getting_started/#main-scripts","text":"Main scripts to be executed lie in the /omc3 module directly. These include: hole_in_one.py to perform frequency analysis on turn by turn BPM data and infer optics (and more) for a given accelerator. madx_wrapper.py to start a MAD-X run with a file or string as input. model_creator.py to provide optics models required for optics analysis. run_kmod.py to analyse data from K-modulation and return the measured optics functions. tbt_converter.py to convert different turn by turn datatypes to sdds, and add noise. amplitude_detuning_analysis.py to perform amp. det. analysis on optics data with tune correction.","title":"Main Scripts"},{"location":"packages/omc3/getting_started/#plotting-scripts","text":"Plotting scripts for analysis outputs can be found in the /omc3/plotting submodule: plot_spectrum.py to generate plots from files generated by frequency analysis. plot_bbq.py to generate plots from files generated by BBQ analysis. plot_amplitude_detuning.py to generate plots from files generated by amplitude detuning analysis. plot_optics_measurements.py to generate plots from files generated by optics_measurements. plot_tfs.py all purpose tfs-file plotter.","title":"Plotting Scripts"},{"location":"packages/omc3/getting_started/#other-scripts","text":"Other general utility scripts are in /omc3/scripts module: update_nattune_in_linfile.py to update the natural tune columns in the lin files by finding the highest peak in the spectrum in a given interval. write_madx_macros.py to generate MAD-X tracking macros with observation points from a twiss file. merge_kmod_results.py to merge lsa_results files created by kmod, and add the luminosity imbalance if the 4 needed IP/Beam files combination are present. A typical analysis workflow with omc3 is described in the next page .","title":"Other Scripts"},{"location":"packages/omc3/know_how/","text":"Note Knowledge about omc3 details and ways to do things. Should include info on all functionality, plus answers to things we mention in meetings or Mattermost.","title":"Know-How"},{"location":"packages/omc3/workflow/","text":"OMC3 Software Workflow \u00b6 Todo Description of a typical analysis workflow. With easy examples for first-timers. Todo Also add smaller examples for individual entry-points, scripts and plotting.","title":"Workflow"},{"location":"packages/omc3/workflow/#omc3-software-workflow","text":"Todo Description of a typical analysis workflow. With easy examples for first-timers. Todo Also add smaller examples for individual entry-points, scripts and plotting.","title":"OMC3 Software Workflow"},{"location":"packages/pylhc/about/","text":"PyLHC Tools \u00b6 This package is a collection of useful scripts and tools for the Optics Measurements and Corrections group (OMC) at CERN. It provides tools which can be useful for working with accelerators, but are not neccessary for optics measurements analysis. Functionality \u00b6 Forced DA Analysis - Script to analyse forced DA. Machine Settings Info - Prints an overview over the machine settings at a given time. HTCondor Job Submitter - Allows to generate jobs based on a templates and submit them to HTCondor. BSRT Logger and BSRT Analysis - Saves data coming straight from LHC BSRT FESA class and allows subsequent analysis. Documentation \u00b6 Autogenerated docs via sphinx . Getting Started \u00b6 This package is Python 3.7+ compatible, but not yet deployed to PyPI. The best way to install is through pip and VCS: git clone https://github.com/pylhc/PyLHC pip install /path/to/PyLHC Or simply from the online master branch, which is stable: pip install git+https://github.com/pylhc/PyLHC.git#egg = pylhc After installing, codes can be run with either python -m pylhc.SCRIPT --FLAG ARGUMENT or calling path to the .py file directly. Note: some of the scripts access functionality only available on the CERN Technical Network. To use those, you should make sure to install the relevant extra dependencies with pip install path/to/Pylhc[tech] .","title":"About"},{"location":"packages/pylhc/about/#pylhc-tools","text":"This package is a collection of useful scripts and tools for the Optics Measurements and Corrections group (OMC) at CERN. It provides tools which can be useful for working with accelerators, but are not neccessary for optics measurements analysis.","title":"PyLHC Tools"},{"location":"packages/pylhc/about/#functionality","text":"Forced DA Analysis - Script to analyse forced DA. Machine Settings Info - Prints an overview over the machine settings at a given time. HTCondor Job Submitter - Allows to generate jobs based on a templates and submit them to HTCondor. BSRT Logger and BSRT Analysis - Saves data coming straight from LHC BSRT FESA class and allows subsequent analysis.","title":"Functionality"},{"location":"packages/pylhc/about/#documentation","text":"Autogenerated docs via sphinx .","title":"Documentation"},{"location":"packages/pylhc/about/#getting-started","text":"This package is Python 3.7+ compatible, but not yet deployed to PyPI. The best way to install is through pip and VCS: git clone https://github.com/pylhc/PyLHC pip install /path/to/PyLHC Or simply from the online master branch, which is stable: pip install git+https://github.com/pylhc/PyLHC.git#egg = pylhc After installing, codes can be run with either python -m pylhc.SCRIPT --FLAG ARGUMENT or calling path to the .py file directly. Note: some of the scripts access functionality only available on the CERN Technical Network. To use those, you should make sure to install the relevant extra dependencies with pip install path/to/Pylhc[tech] .","title":"Getting Started"},{"location":"packages/pylhc/bsrt/","text":"BSRT Logger and BSRT Analysis \u00b6 Todo Description of a typical use-case, with easy examples for first-timers. See the docs for a detailed code description. Analysis \u00b6 Main script to query BRST data, then perform an analysis and output the generated plots. The analysis script runs through the following steps: Processes the output files of BSRT_logger.py for a given time-frame, returns them in as a TfsDataFrame for further processing. Additionally, plots for quick checks of fit parameters, auxiliary variables and beam evolution are generated. If provided a TfsDataFrame file with timestamps, plots of the 2D distribution and comparison of fit parameters to cross sections are added. Logger \u00b6 Script used during Run II to log detailed BSRT data and save it for later analysis. Data from the BSRT for each timestep is put in a dictionary and append to a list . The list is then saved to disk through pickling. Proper testing requires communication with FESA s class, possible only from the Technical Network.","title":"BSRT Logger/Analysis"},{"location":"packages/pylhc/bsrt/#bsrt-logger-and-bsrt-analysis","text":"Todo Description of a typical use-case, with easy examples for first-timers. See the docs for a detailed code description.","title":"BSRT Logger and BSRT Analysis"},{"location":"packages/pylhc/bsrt/#analysis","text":"Main script to query BRST data, then perform an analysis and output the generated plots. The analysis script runs through the following steps: Processes the output files of BSRT_logger.py for a given time-frame, returns them in as a TfsDataFrame for further processing. Additionally, plots for quick checks of fit parameters, auxiliary variables and beam evolution are generated. If provided a TfsDataFrame file with timestamps, plots of the 2D distribution and comparison of fit parameters to cross sections are added.","title":"Analysis"},{"location":"packages/pylhc/bsrt/#logger","text":"Script used during Run II to log detailed BSRT data and save it for later analysis. Data from the BSRT for each timestep is put in a dictionary and append to a list . The list is then saved to disk through pickling. Proper testing requires communication with FESA s class, possible only from the Technical Network.","title":"Logger"},{"location":"packages/pylhc/forced_da/","text":"Forced Dynamic Aperture Analysis \u00b6 Todo Description of a typical use-case, with easy examples for first-timers. See the docs for a detailed code description. Top-level script to run the forced DA analysis, following the procedure described in Felix Carliers Forced DA Paper . Arguments: --Required-- beam (int) : Beam to use. Flags: ['-b', '--beam'] Choices: [1, 2] energy (MultiClass) : Beam energy in GeV. Flags: ['-e', '--energy'] kick_directory (MultiClass) : Analysis kick_directory containing kick files. Flags: ['-k', '--kickdir'] plane (str) : Plane of the kicks. Flags: ['-p', '--plane'] Choices: ['X', 'Y'] --Optional-- emittance_outlier_limit (float) : Limit, i.e. cut from mean, on emittance outliers in meter. Default: 5e-07 emittance_tfs (MultiClass) : Dataframe or Path of pre-saved emittance tfs. emittance_type (str) : Which BSRT data to use (from database). Choices: ['fit_sigma', 'average'] Default: average emittance_window_length (int) : Length of the moving average window. (# data points). Default: 100 fill (int) : Fill that was used. If not given, check out time_around_kicks. Flags: ['-f', '--fill'] fit (str) : Fitting function to use (rearranges parameters to make sense). Choices: ['exponential', 'linear'] Default: exponential intensity_tfs (MultiClass) : Dataframe or Path of pre-saved intensity tfs. intensity_time_after_kick (int) : Defines the times after the kicks (in seconds) which is used for intensity averaging to calculate the losses. Default: [5, 30] intensity_time_before_kick (int) : Defines the times before the kicks (in seconds) which is used for intensity averaging to calculate the losses. Default: [30, 5] normalized_emittance (float) : Assumed NORMALIZED nominal emittance for the machine. Default: 3.7499999999999997e-06 output_directory (MultiClass) : Output kick_directory, if not given subfolder in kick kick_directory Flags: ['-o', '--outdir'] pagestore_db (MultiClass) : (Path to-) presaved timber database show : Show plots. Action: store_true show_wirescan_emittance (BoolOrPathOrDataFrame) : Flag if the emittance from wirescan should also be shown, can also be a Dataframe or Path of pre-saved emittance bws tfs. Default: False timber_db (str) : Which timber database to use. Choices: ['all', 'mdb', 'ldb', 'nxcals'] Default: all time_around_kicks (int) : If no fill is given, this defines the time (in minutes) when data before the first and after the last kick is extracted. Default: 10 plot_styles (str) : Which plotting styles to use, either from omc3 styles or default mpl. Default: ['standard'] manual_style (DictAsString) : Additional style rcParameters which update the set of predefined ones. Default: {}","title":"Forced DA Analysis"},{"location":"packages/pylhc/forced_da/#forced-dynamic-aperture-analysis","text":"Todo Description of a typical use-case, with easy examples for first-timers. See the docs for a detailed code description. Top-level script to run the forced DA analysis, following the procedure described in Felix Carliers Forced DA Paper . Arguments: --Required-- beam (int) : Beam to use. Flags: ['-b', '--beam'] Choices: [1, 2] energy (MultiClass) : Beam energy in GeV. Flags: ['-e', '--energy'] kick_directory (MultiClass) : Analysis kick_directory containing kick files. Flags: ['-k', '--kickdir'] plane (str) : Plane of the kicks. Flags: ['-p', '--plane'] Choices: ['X', 'Y'] --Optional-- emittance_outlier_limit (float) : Limit, i.e. cut from mean, on emittance outliers in meter. Default: 5e-07 emittance_tfs (MultiClass) : Dataframe or Path of pre-saved emittance tfs. emittance_type (str) : Which BSRT data to use (from database). Choices: ['fit_sigma', 'average'] Default: average emittance_window_length (int) : Length of the moving average window. (# data points). Default: 100 fill (int) : Fill that was used. If not given, check out time_around_kicks. Flags: ['-f', '--fill'] fit (str) : Fitting function to use (rearranges parameters to make sense). Choices: ['exponential', 'linear'] Default: exponential intensity_tfs (MultiClass) : Dataframe or Path of pre-saved intensity tfs. intensity_time_after_kick (int) : Defines the times after the kicks (in seconds) which is used for intensity averaging to calculate the losses. Default: [5, 30] intensity_time_before_kick (int) : Defines the times before the kicks (in seconds) which is used for intensity averaging to calculate the losses. Default: [30, 5] normalized_emittance (float) : Assumed NORMALIZED nominal emittance for the machine. Default: 3.7499999999999997e-06 output_directory (MultiClass) : Output kick_directory, if not given subfolder in kick kick_directory Flags: ['-o', '--outdir'] pagestore_db (MultiClass) : (Path to-) presaved timber database show : Show plots. Action: store_true show_wirescan_emittance (BoolOrPathOrDataFrame) : Flag if the emittance from wirescan should also be shown, can also be a Dataframe or Path of pre-saved emittance bws tfs. Default: False timber_db (str) : Which timber database to use. Choices: ['all', 'mdb', 'ldb', 'nxcals'] Default: all time_around_kicks (int) : If no fill is given, this defines the time (in minutes) when data before the first and after the last kick is extracted. Default: 10 plot_styles (str) : Which plotting styles to use, either from omc3 styles or default mpl. Default: ['standard'] manual_style (DictAsString) : Additional style rcParameters which update the set of predefined ones. Default: {}","title":"Forced Dynamic Aperture Analysis"},{"location":"packages/pylhc/job_submitter/","text":"HTCondor Job Sumbitter \u00b6 See the docs for a detailed code description. Description \u00b6 The HTCondor Job sumitter allows to execute a parametric study using a script-mask and a dictionary with parameters to replace, from the command line. The parameters to be replaced must be present in the given mask as %(PARAMETER)s (other types apart from string also allowed). The type of script and executable is freely choosable, but defaults to madx - for which this submitter was originally written. When submitting to HTCondor , data to be transferred back to the working directory must be written in a sub-folder defined by job_output_directory which defaults to Outputdata . This script also allows to check if all HTCondor jobs finished successfully, for resubmissions with a different parameter grid, and for local execution. A Jobs.tfs file is created in the working directory containing the Job Id, parameter per job and job directory for further post processing. --Required-- mask (str) : Script Mask to use replace_dict (DictAsString) : Dict containing the str to replace as keys and values a list of parameters to replace working_directory (str) : Directory where data should be put --Optional-- append_jobs : Flag to rerun job with finer/wider grid, already existing points will not be reexecuted. Action: store_true check_files (str) : List of files/file-name-masks expected to be in the 'job_output_dir' after a successful job (for appending/resuming). Uses the 'glob' function, so unix-wildcards (*) are allowed. If not given, only the presence of the folder itself is checked. dryrun : Flag to only prepare folders and scripts, but does not start madx/submit jobs. Together with resume_jobs this can be use to check which jobs succeeded and which failed. Action: store_true executable (str) : Path to executable or job-type (of ['madx', 'python3', 'python2']) to use. htc_arguments (DictAsString) : Additional arguments for htcondor, as Dict-String. For AccountingGroup please use 'accounting_group'. 'max_retries' and 'notification' have defaults (if not given). Others are just passed on. Default: {} job_output_dir (str) : The name of the output dir of the job. (Make sure your script puts its data there!) Default: Outputdata jobflavour (str) : Jobflavour to give rough estimate of runtime of one job Choices: ('espresso', 'microcentury', 'longlunch', 'workday', 'tomorrow', 'testmatch', 'nextweek') Default: workday jobid_mask (str) : Mask to name jobs from replace_dict num_processes (int) : Number of processes to be used if run locally Default: 4 resume_jobs : Only do jobs that did not work. Action: store_true run_local : Flag to run the jobs on the local machine. Not suggested. Action: store_true script_arguments (DictAsString) : Additional arguments to pass to the script, as dict in key-value pairs ('--' need to be included in the keys). Default: {} script_extension (str) : New extension for the scripts created from the masks. This is inferred automatically for ['madx', 'python3', 'python2']. Otherwise not changed. ssh (str) : Run htcondor from this machine via ssh (needs access to the working_directory ) Example: Tune Sweep \u00b6 In this example we will do a quick submit to HTCondor and starting simulations for both beams over a range of tunes. Python Code from pylhc.job_submitter import main as htcondor_submit from omc3.utils import logging_tools import numpy as np LOG = logging_tools . get_logger ( __name__ ) if __name__ == '__main__' : htcondor_submit ( executable = 'madx' , mask = 'my_madx.mask' , replace_dict = dict ( BEAM = [ 1 , 2 ], TUNEX = np . linspace ( 62.3 , 62.32 , 11 ) . tolist (), TUNEY = np . linspace ( 60.31 , 60.33 , 11 ) . tolist (), ), jobid_mask = \"b %(BEAM)d .qx %(TUNEX)s .qy %(TUNEY)s \" , working_directory = '/afs/cern.ch/work/u/username/study.tune_sweep' , ssh = 'lxplus.cern.ch' ) my_madx.mask !############################## Create Soft Links and Directories ################################################################ option, warn,info; system, \"mkdir Outputdata\"; system, \"ln -fns /afs/cern.ch/eng/lhc/optics/V6.503 db5\"; system, \"ln -fns /afs/cern.ch/eng/lhc/optics/SLHCV1.0 slhc\"; system, \"ln -fns /afs/cern.ch/eng/lhc/optics/runII/2018 optics2018\"; !option, -echo,warn,-info; !############################## Make macros available ############################################################################ call,file=\"optics2018/toolkit/macro.madx\"; !############################## Beam Options ##################################################################################### mylhcbeam=%(BEAM)s; ! Beam to use. HINT: mylhcbeam variable is used in some macros as well!! qxinit=%(TUNEX)s; qyinit=%(TUNEY)s; emittance=7.29767146889e-09; Nb_0=1.0e10; ! number of particles in beam !############################## Set up Lattice ################################################################################### call,file=\"optics2018/lhc_as-built.seq\"; ! Definine the optics: call, file=\"optics2018/PROTON/opticsfile.1\"; ! Basic optics setup (injection optics) call, file=\"optics2018/PROTON/opticsfile.22_ctpps2\"; ! Redefine Optics to Round 30cm collision optics beam, sequence=lhcb1, bv= 1, energy=NRJ, particle=proton, npart=Nb_0, kbunch=1, ex=emittance,ey=emittance; beam, sequence=lhcb2, bv=-1, energy=NRJ, particle=proton, npart=Nb_0, kbunch=1, ex=emittance,ey=emittance; !############################## Tune Matching and Output ######################################################################### use, sequence=lhcb%(BEAM)s; match,chrom; global, q1=qxinit, q2=qyinit; vary, name=dQx.b%(BEAM)s, step=1.0E-7 ; vary, name=dQy.b%(BEAM)s, step=1.0E-7 ; lmdif, calls=100, tolerance=1.0E-21; endmatch; select, flag=twiss, clear; select, flag=twiss, pattern=\"BPM\", column=name,s,x,y,betx,bety,alfx,alfy,dx,dpx,mux,muy; select, flag=twiss, pattern=\"M\", column=name,s,x,y,betx,bety,alfx,alfy,dx,dpx,mux,muy; select, flag=twiss, pattern=\"IP\", column=name,s,x,y,betx,bety,alfx,alfy,dx,dpx,mux,muy; twiss, chrom, file='Outputdata/b%(BEAM)s.twiss.tfs'; After executing the python script, we can check the status of our jobs via condor_q (on the ssh server if not locally setup). The result should look something like this: condor_q : Shows the jobs in the condor queue. At the same time, a folder structure has developed in the given working directory : In the main directory will be the Job.tfs , were each row corresponds to instance of parameters filled in and contains infos that. The first column will be JobId which is either just a numbering of the jobs, or - in our case - the filled in jobid_mask . The next columns are the parameters given, here: BEAM , TUNEX and TUNEY . They contain the value the respective job-instance as filled in. JobDirectory contains the path to the job directory, while JobFile is the name of the file and ShellScript the name of the script within this directory, which are used to run the job. The queuehtc.sub is the submission file, which tells HTCondor the paths to all the ShellScripts to run. In the JobDirectories , also named according to the jobid_mask , there will be some files present already: The ShellScript Jobid .sh , which contains commands to create the job_output_dir and the madx command to run the script And the JobFile which is the original mask , parameter values filled in. All of these files are created before the submit to HTCondor. After all jobs have finished, htcondor.___.err , htcondor.___.log and htcondor.___.out files for the respective job are transferred to these directories, containing the printouts of the outputstreams of the job. And finally, also the job_output_dir , here Outputdata containing the calculated .twiss -file, will be in there as well, ready for post-processing. Pitfalls and Hints \u00b6 when using the ssh option, the working directory needs to be accessible from both, the local machine as well as the ssh-server. ssh is only used for the final commit command to send the jobs to HTCondor, as this is quite tricky to setup on a local machine. the run_local option is only not suggested, as your jobs will obviously not be parallelized. If you have only short jobs, a small parameter space or are already on a powerful machine, you can use this option of course.","title":"Job Submitter"},{"location":"packages/pylhc/job_submitter/#htcondor-job-sumbitter","text":"See the docs for a detailed code description.","title":"HTCondor Job Sumbitter"},{"location":"packages/pylhc/job_submitter/#description","text":"The HTCondor Job sumitter allows to execute a parametric study using a script-mask and a dictionary with parameters to replace, from the command line. The parameters to be replaced must be present in the given mask as %(PARAMETER)s (other types apart from string also allowed). The type of script and executable is freely choosable, but defaults to madx - for which this submitter was originally written. When submitting to HTCondor , data to be transferred back to the working directory must be written in a sub-folder defined by job_output_directory which defaults to Outputdata . This script also allows to check if all HTCondor jobs finished successfully, for resubmissions with a different parameter grid, and for local execution. A Jobs.tfs file is created in the working directory containing the Job Id, parameter per job and job directory for further post processing. --Required-- mask (str) : Script Mask to use replace_dict (DictAsString) : Dict containing the str to replace as keys and values a list of parameters to replace working_directory (str) : Directory where data should be put --Optional-- append_jobs : Flag to rerun job with finer/wider grid, already existing points will not be reexecuted. Action: store_true check_files (str) : List of files/file-name-masks expected to be in the 'job_output_dir' after a successful job (for appending/resuming). Uses the 'glob' function, so unix-wildcards (*) are allowed. If not given, only the presence of the folder itself is checked. dryrun : Flag to only prepare folders and scripts, but does not start madx/submit jobs. Together with resume_jobs this can be use to check which jobs succeeded and which failed. Action: store_true executable (str) : Path to executable or job-type (of ['madx', 'python3', 'python2']) to use. htc_arguments (DictAsString) : Additional arguments for htcondor, as Dict-String. For AccountingGroup please use 'accounting_group'. 'max_retries' and 'notification' have defaults (if not given). Others are just passed on. Default: {} job_output_dir (str) : The name of the output dir of the job. (Make sure your script puts its data there!) Default: Outputdata jobflavour (str) : Jobflavour to give rough estimate of runtime of one job Choices: ('espresso', 'microcentury', 'longlunch', 'workday', 'tomorrow', 'testmatch', 'nextweek') Default: workday jobid_mask (str) : Mask to name jobs from replace_dict num_processes (int) : Number of processes to be used if run locally Default: 4 resume_jobs : Only do jobs that did not work. Action: store_true run_local : Flag to run the jobs on the local machine. Not suggested. Action: store_true script_arguments (DictAsString) : Additional arguments to pass to the script, as dict in key-value pairs ('--' need to be included in the keys). Default: {} script_extension (str) : New extension for the scripts created from the masks. This is inferred automatically for ['madx', 'python3', 'python2']. Otherwise not changed. ssh (str) : Run htcondor from this machine via ssh (needs access to the working_directory )","title":"Description"},{"location":"packages/pylhc/job_submitter/#example-tune-sweep","text":"In this example we will do a quick submit to HTCondor and starting simulations for both beams over a range of tunes. Python Code from pylhc.job_submitter import main as htcondor_submit from omc3.utils import logging_tools import numpy as np LOG = logging_tools . get_logger ( __name__ ) if __name__ == '__main__' : htcondor_submit ( executable = 'madx' , mask = 'my_madx.mask' , replace_dict = dict ( BEAM = [ 1 , 2 ], TUNEX = np . linspace ( 62.3 , 62.32 , 11 ) . tolist (), TUNEY = np . linspace ( 60.31 , 60.33 , 11 ) . tolist (), ), jobid_mask = \"b %(BEAM)d .qx %(TUNEX)s .qy %(TUNEY)s \" , working_directory = '/afs/cern.ch/work/u/username/study.tune_sweep' , ssh = 'lxplus.cern.ch' ) my_madx.mask !############################## Create Soft Links and Directories ################################################################ option, warn,info; system, \"mkdir Outputdata\"; system, \"ln -fns /afs/cern.ch/eng/lhc/optics/V6.503 db5\"; system, \"ln -fns /afs/cern.ch/eng/lhc/optics/SLHCV1.0 slhc\"; system, \"ln -fns /afs/cern.ch/eng/lhc/optics/runII/2018 optics2018\"; !option, -echo,warn,-info; !############################## Make macros available ############################################################################ call,file=\"optics2018/toolkit/macro.madx\"; !############################## Beam Options ##################################################################################### mylhcbeam=%(BEAM)s; ! Beam to use. HINT: mylhcbeam variable is used in some macros as well!! qxinit=%(TUNEX)s; qyinit=%(TUNEY)s; emittance=7.29767146889e-09; Nb_0=1.0e10; ! number of particles in beam !############################## Set up Lattice ################################################################################### call,file=\"optics2018/lhc_as-built.seq\"; ! Definine the optics: call, file=\"optics2018/PROTON/opticsfile.1\"; ! Basic optics setup (injection optics) call, file=\"optics2018/PROTON/opticsfile.22_ctpps2\"; ! Redefine Optics to Round 30cm collision optics beam, sequence=lhcb1, bv= 1, energy=NRJ, particle=proton, npart=Nb_0, kbunch=1, ex=emittance,ey=emittance; beam, sequence=lhcb2, bv=-1, energy=NRJ, particle=proton, npart=Nb_0, kbunch=1, ex=emittance,ey=emittance; !############################## Tune Matching and Output ######################################################################### use, sequence=lhcb%(BEAM)s; match,chrom; global, q1=qxinit, q2=qyinit; vary, name=dQx.b%(BEAM)s, step=1.0E-7 ; vary, name=dQy.b%(BEAM)s, step=1.0E-7 ; lmdif, calls=100, tolerance=1.0E-21; endmatch; select, flag=twiss, clear; select, flag=twiss, pattern=\"BPM\", column=name,s,x,y,betx,bety,alfx,alfy,dx,dpx,mux,muy; select, flag=twiss, pattern=\"M\", column=name,s,x,y,betx,bety,alfx,alfy,dx,dpx,mux,muy; select, flag=twiss, pattern=\"IP\", column=name,s,x,y,betx,bety,alfx,alfy,dx,dpx,mux,muy; twiss, chrom, file='Outputdata/b%(BEAM)s.twiss.tfs'; After executing the python script, we can check the status of our jobs via condor_q (on the ssh server if not locally setup). The result should look something like this: condor_q : Shows the jobs in the condor queue. At the same time, a folder structure has developed in the given working directory : In the main directory will be the Job.tfs , were each row corresponds to instance of parameters filled in and contains infos that. The first column will be JobId which is either just a numbering of the jobs, or - in our case - the filled in jobid_mask . The next columns are the parameters given, here: BEAM , TUNEX and TUNEY . They contain the value the respective job-instance as filled in. JobDirectory contains the path to the job directory, while JobFile is the name of the file and ShellScript the name of the script within this directory, which are used to run the job. The queuehtc.sub is the submission file, which tells HTCondor the paths to all the ShellScripts to run. In the JobDirectories , also named according to the jobid_mask , there will be some files present already: The ShellScript Jobid .sh , which contains commands to create the job_output_dir and the madx command to run the script And the JobFile which is the original mask , parameter values filled in. All of these files are created before the submit to HTCondor. After all jobs have finished, htcondor.___.err , htcondor.___.log and htcondor.___.out files for the respective job are transferred to these directories, containing the printouts of the outputstreams of the job. And finally, also the job_output_dir , here Outputdata containing the calculated .twiss -file, will be in there as well, ready for post-processing.","title":"Example: Tune Sweep"},{"location":"packages/pylhc/job_submitter/#pitfalls-and-hints","text":"when using the ssh option, the working directory needs to be accessible from both, the local machine as well as the ssh-server. ssh is only used for the final commit command to send the jobs to HTCondor, as this is quite tricky to setup on a local machine. the run_local option is only not suggested, as your jobs will obviously not be parallelized. If you have only short jobs, a small parameter space or are already on a powerful machine, you can use this option of course.","title":"Pitfalls and Hints"},{"location":"packages/pylhc/machine_settings_info/","text":"Machine Settings Info \u00b6 Todo Description of a typical use-case, with easy examples for first-timers. See the docs for a detailed code description. Prints an overview over the machine settings at a provided given time, or the current settings if no time is given. Can be run from command line, parameters as given in :meth: print_machine_settings_overview.main . print_machine_settings_overview.py [ -h ] [ --time TIME ] [ --knobs KNOBS [ KNOBS ... ]] [ --bp_regexp BP_REGEXP ] [ --accel ACCEL ] [ --out OUT ] optional arguments: -h, --help show this help message and exit --time TIME, -t TIME Time as 'Y-m-d H:M:S.f' format. --knobs KNOBS [ KNOBS ... ] , -k KNOBS [ KNOBS ... ] List of knobnames. --bp_regexp BP_REGEXP, -r BP_REGEXP Beamprocess regexp filter. --accel ACCEL, -a ACCEL Accelerator name. --out OUT, -o OUT Output path.","title":"Machine Settings Info"},{"location":"packages/pylhc/machine_settings_info/#machine-settings-info","text":"Todo Description of a typical use-case, with easy examples for first-timers. See the docs for a detailed code description. Prints an overview over the machine settings at a provided given time, or the current settings if no time is given. Can be run from command line, parameters as given in :meth: print_machine_settings_overview.main . print_machine_settings_overview.py [ -h ] [ --time TIME ] [ --knobs KNOBS [ KNOBS ... ]] [ --bp_regexp BP_REGEXP ] [ --accel ACCEL ] [ --out OUT ] optional arguments: -h, --help show this help message and exit --time TIME, -t TIME Time as 'Y-m-d H:M:S.f' format. --knobs KNOBS [ KNOBS ... ] , -k KNOBS [ KNOBS ... ] List of knobnames. --bp_regexp BP_REGEXP, -r BP_REGEXP Beamprocess regexp filter. --accel ACCEL, -a ACCEL Accelerator name. --out OUT, -o OUT Output path.","title":"Machine Settings Info"},{"location":"procedures/ampdet/","text":"Amplitude Detuning Measurements \u00b6 This is only an example procedure for demonstration purposes, to be written. Measurement \u00b6 Correct Coupling Kick at medium amplitude and wait for the result. Then put the result into the coupling correction tool. Check Coupling Just in case the previous step did not work. If it really failed: Crawl under a table and cry. You are a disgrace for the OMC-Team. Everyone can correct coupling. Duh. Do some measurement Do kicks, then do some more kicks, and then even more kicks and when you think you are done, better kick some more just for good measure. Let's just hope you won't be the one who has to analyze all this data... This is a subtask I don't actually have anything to say. I just wanted to show that these also work. Step 2 Step 3 Kicks in Vertical Plane Step 1 Step 2 Step 3 Analysis \u00b6 Load data Check data for correct tunes Analyse data Run BBQ correction","title":"Amplitude Detuning"},{"location":"procedures/ampdet/#amplitude-detuning-measurements","text":"This is only an example procedure for demonstration purposes, to be written.","title":"Amplitude Detuning Measurements"},{"location":"procedures/ampdet/#measurement","text":"Correct Coupling Kick at medium amplitude and wait for the result. Then put the result into the coupling correction tool. Check Coupling Just in case the previous step did not work. If it really failed: Crawl under a table and cry. You are a disgrace for the OMC-Team. Everyone can correct coupling. Duh. Do some measurement Do kicks, then do some more kicks, and then even more kicks and when you think you are done, better kick some more just for good measure. Let's just hope you won't be the one who has to analyze all this data... This is a subtask I don't actually have anything to say. I just wanted to show that these also work. Step 2 Step 3 Kicks in Vertical Plane Step 1 Step 2 Step 3","title":"Measurement"},{"location":"procedures/ampdet/#analysis","text":"Load data Check data for correct tunes Analyse data Run BBQ correction","title":"Analysis"},{"location":"procedures/info/","text":"Checklists \u00b6 In this section there are multiple checklists for procedures, that can be used as guidance during measurements. To mark a task as finished, just click on the checkbox next to it! Non-Persistance The checked-boxes stay checked only as long as you stay on that page! Any reload will reset the ticks. Log Your Work! After performing measurements or tests in the CCC , remember to add an entry into the LogBook. It can be accessed through the OP Webtools .","title":"Info"},{"location":"procedures/info/#checklists","text":"In this section there are multiple checklists for procedures, that can be used as guidance during measurements. To mark a task as finished, just click on the checkbox next to it! Non-Persistance The checked-boxes stay checked only as long as you stay on that page! Any reload will reset the ticks. Log Your Work! After performing measurements or tests in the CCC , remember to add an entry into the LogBook. It can be accessed through the OP Webtools .","title":"Checklists"},{"location":"procedures/kmod/","text":"Measuring \\(\\beta^{*}\\) Using K-Modulation in the LHC \u00b6 The important stuff. This decides over lumi and lives in the experiments. Measurement \u00b6 Adjust Working Point The tunes should be moved to a working point with a large tune separation, such as \\(Q_x = 0.28 / Q_y = 0.31\\) , to allow for maximum modulation amplitude. Check Coupling Perform quick check for \\(|C^{-}|\\) to avoid influence from a possible closest tune approach. Also check for any unwanted local coupling bumps around the modulated quadrupole. Check feedbacks Turn on orbit feedback In case of any (design) orbit excursion in the quadrupoles, enable orbit feedback to avoid a change of the CO around the ring. Caveat: for the determination of the crossing angles, orbit feedback should be off. Turn off tune feedback Otherwise modulation and feedback would work against each other. Run K-Modulation Fire up the K-Mod application . There two options are available: IP Modulation : Runs a modulation on both quadrupoles closest to the selected IP. Single circuit modulation : Runs a modulation on a selected quadrupole circuit (used for measuring the beta-functions in IR4, where BSRT is located). Chose Modulation Amplitude Choose a modulation current such that the change in tune is roughly 0.01. This can either be done by looking up old shifts with similar optics or by increasing the amplitude until satisfactory tune change is observed. Modulation frequency is chosen by the system, with higher modulation amplitude resulting in lower modulation frequency. Document Measurement As no automatic logging of the modulation is implemented for now, parameters should be logged in the logbook. Parameters to log are: Starttime , Endtime , Modulation current , IP , other comments such as \\(\\beta^{*}\\) , status of the OFB , is significant tunejitter/-jump observed. Analysis \u00b6 Extract Data from Timber After the analysis, a window should open to allow for extraction of the data from Timber . Alternatively, Extract previous trim can be used. Saving in a separate directory with a descriptive name is recommended (e.g. Kmod_IPX_beta_beforeCorrection_starttime ) and should be added to the modulation logbook entry. Start the Analysis Run the python codes on the extracted Timber data to get the \\(\\beta\\) you need. As of now, only the Kmod analysis from Beta-Beat.src can be called from the K-Modulation GUI for the case of an analysis of an IP-Modulation. Codes and some documentation may be found for Python2 and for Python3 . Check Results The results of the analysis should be located in the previously specified working directory and can be checked by eye using a text editor of choice. Use in Beta-Beat GUI for Correction Using this script , the results can be brought in a form which is readable for the BBGUI and can then be used to calculate a correction. Publish Results If results are satisfactory, both Python2 and Python3 should create a file called lsa_results.tfs , which can be uploaded using the LSA optics uploader for other users to access data.","title":"K Modulation"},{"location":"procedures/kmod/#measuring-beta-using-k-modulation-in-the-lhc","text":"The important stuff. This decides over lumi and lives in the experiments.","title":"Measuring \\(\\beta^{*}\\) Using K-Modulation in the LHC"},{"location":"procedures/kmod/#measurement","text":"Adjust Working Point The tunes should be moved to a working point with a large tune separation, such as \\(Q_x = 0.28 / Q_y = 0.31\\) , to allow for maximum modulation amplitude. Check Coupling Perform quick check for \\(|C^{-}|\\) to avoid influence from a possible closest tune approach. Also check for any unwanted local coupling bumps around the modulated quadrupole. Check feedbacks Turn on orbit feedback In case of any (design) orbit excursion in the quadrupoles, enable orbit feedback to avoid a change of the CO around the ring. Caveat: for the determination of the crossing angles, orbit feedback should be off. Turn off tune feedback Otherwise modulation and feedback would work against each other. Run K-Modulation Fire up the K-Mod application . There two options are available: IP Modulation : Runs a modulation on both quadrupoles closest to the selected IP. Single circuit modulation : Runs a modulation on a selected quadrupole circuit (used for measuring the beta-functions in IR4, where BSRT is located). Chose Modulation Amplitude Choose a modulation current such that the change in tune is roughly 0.01. This can either be done by looking up old shifts with similar optics or by increasing the amplitude until satisfactory tune change is observed. Modulation frequency is chosen by the system, with higher modulation amplitude resulting in lower modulation frequency. Document Measurement As no automatic logging of the modulation is implemented for now, parameters should be logged in the logbook. Parameters to log are: Starttime , Endtime , Modulation current , IP , other comments such as \\(\\beta^{*}\\) , status of the OFB , is significant tunejitter/-jump observed.","title":"Measurement"},{"location":"procedures/kmod/#analysis","text":"Extract Data from Timber After the analysis, a window should open to allow for extraction of the data from Timber . Alternatively, Extract previous trim can be used. Saving in a separate directory with a descriptive name is recommended (e.g. Kmod_IPX_beta_beforeCorrection_starttime ) and should be added to the modulation logbook entry. Start the Analysis Run the python codes on the extracted Timber data to get the \\(\\beta\\) you need. As of now, only the Kmod analysis from Beta-Beat.src can be called from the K-Modulation GUI for the case of an analysis of an IP-Modulation. Codes and some documentation may be found for Python2 and for Python3 . Check Results The results of the analysis should be located in the previously specified working directory and can be checked by eye using a text editor of choice. Use in Beta-Beat GUI for Correction Using this script , the results can be brought in a form which is readable for the BBGUI and can then be used to calculate a correction. Publish Results If results are satisfactory, both Python2 and Python3 should create a file called lsa_results.tfs , which can be uploaded using the LSA optics uploader for other users to access data.","title":"Analysis"},{"location":"procedures/xing_scan/","text":"","title":"Crossing Angle Scan"},{"location":"resources/faqs/","text":"Gradually include here different pieces of useful information, possibly from questions answered in meetings or on Mattermost, which don't really fit anywhere else","title":"FAQ"},{"location":"resources/links/","text":"Links to Various Useful Resources \u00b6 Hyperlinks Legend Publicly Accessible Page Webpage needs CERN login Only accessible from the CERN Network OMC \u00b6 OMC Mattermost Meetings (OMC Team Indico) Rogelio's Website GUI Links \u00b6 Beta-Beat Beta-Beat Gitlab Jira Bugtracker Artifactory Kmod Kmod Gitlab Artifactory Multiturn Multiturn Gitlab Artifactory JWS Programs \u00b6 All of these are only accessible from CERN Network . Beta-Beat PRO jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp DEV jws https://bewww.cern.ch/ap/deployments-dev/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp Kmod jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-kmod/PRO/lhc-app-kmod-lhc-app-kmod.jnlp Multiturn jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-multiturn/PRO/lhc-multiturn-lhc-multiturn.jnlp LSA-App-Suite jws http://bewww.cern.ch/ap/deployments/applications/cern/lsa/lsa-app-suite/PRO/lsa-app-suite-lhc.jnlpx LHC Panel open jws 'http://bewww.cern.ch/ap/deployments/applications/cern/lsa/lsa-app-suite/PRO/lsa-app-suite-lhc.jnlpx?accelerator=LHC&lsa.server=lhc&lsa.contextfamily=BP&arg0=lsa-app-settings-management' Timber jws http://bewww.cern.ch/ap/deployments/applications/cern/accsoft/cals/accsoft-cals-extr-app/PRO/timber.jnlpx Computer Setup at CERN \u00b6 Computing Accounts Management CERN Resources Portal AFS and Kerberos (for Ubuntu) AFS and Kerberos (for WSL) Using Kerberos for SSH Teleworking \u00b6 CodiMD CERN - Teleworking Tips & Tricks Cern Computing Blog - Useful tools for teleworking Remote Desktop Service (or with application, connect to cernts.cern.ch ) Computing setup for Members \u00b6 LHC Data Sources HTCondor Batch Docs Setup HTCondor for local use More HTCondor hints in the Python wrapper CERN \u00b6 Webtools \u00b6 Vistars HTCondor Grafana OP Webtools Page Timber INCA and LSA Applications LHC MD webpage Beam Performance Tracking Site SWAN (Jupyter Notebooks in CERN Cloud) CERN OpenStack (Virtual Machines in Cloud) Room Booking Info \u00b6 LHC Naming Conventions - Equipment Codes Beam-Beam and Luminosity Studies Accelerating Python Wiki CERN CBNG Manual LSA Wiki Repositories \u00b6 LHC Optics Repository CERN Optics Repository Development Guidelines and How-To's \u00b6 Markdown Cheatsheet Jira Text Formatting Notation Git \u00b6 Generating SSH keys for GitHub Git - configuration for symbolic links instead of files GitHub Flavored Markdown Python \u00b6 Python Style Guide Python docs Java \u00b6 Java Guidelines","title":"Useful Links"},{"location":"resources/links/#links-to-various-useful-resources","text":"Hyperlinks Legend Publicly Accessible Page Webpage needs CERN login Only accessible from the CERN Network","title":"Links to Various Useful Resources"},{"location":"resources/links/#omc","text":"OMC Mattermost Meetings (OMC Team Indico) Rogelio's Website","title":"OMC"},{"location":"resources/links/#gui-links","text":"Beta-Beat Beta-Beat Gitlab Jira Bugtracker Artifactory Kmod Kmod Gitlab Artifactory Multiturn Multiturn Gitlab Artifactory","title":"GUI Links"},{"location":"resources/links/#jws-programs","text":"All of these are only accessible from CERN Network . Beta-Beat PRO jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp DEV jws https://bewww.cern.ch/ap/deployments-dev/applications/cern/lhc/lhc-app-betabeating/PRO/BetaBeating-Control-3t.jnlp Kmod jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-app-kmod/PRO/lhc-app-kmod-lhc-app-kmod.jnlp Multiturn jws https://bewww.cern.ch/ap/deployments/applications/cern/lhc/lhc-multiturn/PRO/lhc-multiturn-lhc-multiturn.jnlp LSA-App-Suite jws http://bewww.cern.ch/ap/deployments/applications/cern/lsa/lsa-app-suite/PRO/lsa-app-suite-lhc.jnlpx LHC Panel open jws 'http://bewww.cern.ch/ap/deployments/applications/cern/lsa/lsa-app-suite/PRO/lsa-app-suite-lhc.jnlpx?accelerator=LHC&lsa.server=lhc&lsa.contextfamily=BP&arg0=lsa-app-settings-management' Timber jws http://bewww.cern.ch/ap/deployments/applications/cern/accsoft/cals/accsoft-cals-extr-app/PRO/timber.jnlpx","title":"JWS Programs"},{"location":"resources/links/#computer-setup-at-cern","text":"Computing Accounts Management CERN Resources Portal AFS and Kerberos (for Ubuntu) AFS and Kerberos (for WSL) Using Kerberos for SSH","title":"Computer Setup at CERN"},{"location":"resources/links/#teleworking","text":"CodiMD CERN - Teleworking Tips & Tricks Cern Computing Blog - Useful tools for teleworking Remote Desktop Service (or with application, connect to cernts.cern.ch )","title":"Teleworking"},{"location":"resources/links/#computing-setup-for-members","text":"LHC Data Sources HTCondor Batch Docs Setup HTCondor for local use More HTCondor hints in the Python wrapper","title":"Computing setup for Members"},{"location":"resources/links/#cern","text":"","title":"CERN"},{"location":"resources/links/#webtools","text":"Vistars HTCondor Grafana OP Webtools Page Timber INCA and LSA Applications LHC MD webpage Beam Performance Tracking Site SWAN (Jupyter Notebooks in CERN Cloud) CERN OpenStack (Virtual Machines in Cloud) Room Booking","title":"Webtools"},{"location":"resources/links/#info","text":"LHC Naming Conventions - Equipment Codes Beam-Beam and Luminosity Studies Accelerating Python Wiki CERN CBNG Manual LSA Wiki","title":"Info"},{"location":"resources/links/#repositories","text":"LHC Optics Repository CERN Optics Repository","title":"Repositories"},{"location":"resources/links/#development-guidelines-and-how-tos","text":"Markdown Cheatsheet Jira Text Formatting Notation","title":"Development Guidelines and How-To's"},{"location":"resources/links/#git","text":"Generating SSH keys for GitHub Git - configuration for symbolic links instead of files GitHub Flavored Markdown","title":"Git"},{"location":"resources/links/#python","text":"Python Style Guide Python docs","title":"Python"},{"location":"resources/links/#java","text":"Java Guidelines","title":"Java"},{"location":"resources/publications/","text":"OMC Publications \u00b6 Journal Publications \u00b6 2020 \u00b6 Detection of faulty beam position monitors using unsupervised learning, E. Fol, R. Tom\u00e1s, J. Coello de Portugal, and G. Franchetti Phys. Rev. Accel. Beams 23, 102805, 2020 @article{faulty_bpm_detection, title = {Detection of faulty beam position monitors using unsupervised learning}, author = {Fol, E. and Tom\\'as, R. and Coello de Portugal, J. and Franchetti, G.}, journal = {Phys. Rev. Accel. Beams}, volume = {23}, issue = {10}, pages = {102805}, numpages = {10}, year = {2020}, month = {Oct}, publisher = {American Physical Society}, doi = {10.1103/PhysRevAccelBeams.23.102805}, url = {https://link.aps.org/doi/10.1103/PhysRevAccelBeams.23.102805} } 2019 \u00b6 2018 \u00b6 Strategy Reports \u00b6 MD Notes \u00b6 2020 \u00b6 2019 \u00b6 2018 \u00b6","title":"Publications"},{"location":"resources/publications/#omc-publications","text":"","title":"OMC Publications"},{"location":"resources/publications/#journal-publications","text":"","title":"Journal Publications"},{"location":"resources/publications/#2020","text":"Detection of faulty beam position monitors using unsupervised learning, E. Fol, R. Tom\u00e1s, J. Coello de Portugal, and G. Franchetti Phys. Rev. Accel. Beams 23, 102805, 2020 @article{faulty_bpm_detection, title = {Detection of faulty beam position monitors using unsupervised learning}, author = {Fol, E. and Tom\\'as, R. and Coello de Portugal, J. and Franchetti, G.}, journal = {Phys. Rev. Accel. Beams}, volume = {23}, issue = {10}, pages = {102805}, numpages = {10}, year = {2020}, month = {Oct}, publisher = {American Physical Society}, doi = {10.1103/PhysRevAccelBeams.23.102805}, url = {https://link.aps.org/doi/10.1103/PhysRevAccelBeams.23.102805} }","title":"2020"},{"location":"resources/publications/#2019","text":"","title":"2019"},{"location":"resources/publications/#2018","text":"","title":"2018"},{"location":"resources/publications/#strategy-reports","text":"","title":"Strategy Reports"},{"location":"resources/publications/#md-notes","text":"","title":"MD Notes"},{"location":"resources/publications/#2020_1","text":"","title":"2020"},{"location":"resources/publications/#2019_1","text":"","title":"2019"},{"location":"resources/publications/#2018_1","text":"","title":"2018"},{"location":"resources/howto/setup/","text":"Useful Setup How-To's \u00b6 Mounting TN Resources on GPN and Other Machines \u00b6 To be able to run the GUI or CBNG seamlessly from computers which are not in the technical network, it might be useful to mount /user , /nfs and /eos via sshfs using the following recipe: Create mountpoints and symbolic links (only once) mkdir -p ~/mnt/user && ln -nfs ~/mnt/user /user mkdir ~/mnt/nfs && ln -nfs ~/mnt/nfs /nfs mkdir ~/mnt/eos && ln -nfs ~/mnt/eos /eos Mount network resources (repeat after timeouts and restarts) sshfs username@cs-ccr-dev3.cern.ch:/user/ ~/mnt/user sshfs username@cs-ccr-dev3.cern.ch:/nfs/ ~/mnt/nfs sshfs username@lxplus.cern.ch:/eos/ ~/mnt/eos If outside of the GPN , jump through lxplus to mount dev3 -folders: sshfs username@cs-ccr-dev3.cern.ch:/user/ ~/mnt/user -o ssh_command = 'ssh -t username@lxplus.cern.ch ssh' sshfs username@cs-ccr-dev3.cern.ch:/nfs/ ~/mnt/nfs -o ssh_command = 'ssh -t username@lxplus.cern.ch ssh' To avoid getting asked for your password all the time, you should have your ssh properly configured with Kerberos. In case you need to unmount these sudo fusermount -u ~/mnt/user sudo fusermount -u ~/mnt/nfs sudo fusermount -u ~/mnt/eos Running GUIs Locally \u00b6 To use the KMod GUI or the KnobPanel in the Beta-Beat GUI , it is required to be on the TN , as they need to connect to LSA . If you are in the GPN but not on the TN , you will need to tunnel through some machines. First, install the program sshuttle , which should be available in your package manager. Then, run this command in a terminal and leave it open: sshuttle -vr <username>@cs-ccr-dev2 172 .18.0.0/16 All traffic related to the technical network will be redirected through the cs-ccr-dev2 machine which has access to both networks. In case it isn't available, the other cs-ccr-devX machines can be used. Configuring Gitlab CI to Automatically Pull into AFS \u00b6 If you are programming locally, but also want to have a copy on AFS , either because your colleges are not comfortable with Gitlab or you need the code for other scripts that you are running on lxplus or similar, here is how: Create Service Account \u00b6 Security Risk! We will be creating a new account, so you do not have to add the password of your main account to your Gitlab repository : this would be a security risk! Admittedly, it is a low one if everything is done correctly, but just in case something goes wrong or leaks, you can just delete this account . Also, you can give this account only the rights necessary to write to AFS in the first place and do not risk that anyone can access other sensible information. 1. Create a new service account, via the CERN account management , of length 8. You need at least 8 characters to mask that name in Gitlab (if you want to do so), but on the other hand, there is a warning, that AFS cannot handle names longer than 8. So exactly 8 seems to be the sweetspot. If you do not care about masking the name (it is not as important as masking the password, see below) you can go shorter. 2. Set a password of length \u2265 8 containing only [a-zA-Z0-9+/@:] . We want to hide the password later in gitlab, and this is only possible if its >= 8 characters long and only contains Base64 characters as well as @: . You can go as long as you want. Better make this 15 characters long at least. 3. Activate the AFS service for the newly created account. Setup AFS folder \u00b6 Steps to be done on AFS : 4. Clone your git into your AFS destination. It is important here to you use the right link to set this up, depending on whether you want your repository to be private or public. Go to your Gitlab repository page and click onto the blue clone button on the right. If it is okay for your repo to be publicly accessible you can use the HTTPS url. If you want to set it up privately you will need to clone with the KRB5 url, so Kerberos can be later used to authenticate your service account. ( See also below for giving it rights to the repository.) 5. Run: find . -type d -exec fs sa {} ACCOUNTNAME rlidw \\; Run within the repository or replace . with the repository name! Replace ACCOUNTNAME with the name of your Service Account. This will give read/write access to this repository to the service account. Just giving these rights to the top-folder will not work, as it does not propagate to subfolders automatically. A possible other way would be to create an empty folder and give writing rights to that one to the service account and then login to lxplus with the service account and clone the repository directly with that account. The latter is a good test to see if everything worked correctly anyway. Do not forget, if you have set up the repository with Kerberos authentification, to adapt your .ssh/config to delegate the credentials. Repeated for easy copying: find . -type d -exec fs sa {} ACCOUNTNAME rlidw \\; Setup Git \u00b6 Steps to be done on your Gitlab repository: 6. Give your service account access rights or set the repository to public. The former can be done under Members , the latter under Settings \u2192 General \u2192 Visibility, project features, permissions . If you set it up to be a private repository, you need to have cloned the repository with the KRB5 url ( see above ). 7. Add SERVICE_ACCOUNT_USERNAME and SERVICE_ACCOUNT_PASSWORD variables. This is done in Settings \u2192 CI/CD \u2192 Variables . Obviously the values of the variables will be the username and password of your newly created service account respectively. These variables will be used in the next step in the YAML. For obvious reasons, DO NOT WRITE THE PASSWORD IN CLEAR TEXT INTO THE YAML . Security Risk! MAKE SURE AT LEAST THE SERVICE_ACCOUNT_PASSWORD VARIABLE IS SET TO Masked !!! 8. Add a new stage to your .gitlab-ci.yml file, or create a new one. stages : - afs_pull afs : stage : afs_pull image : gitlab-registry.cern.ch/linuxsupport/cc7-base before_script : - yum install -y openssh-clients - mkdir -p ~/.ssh - 'echo \"lxplus ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTA/5AzXgbkSapknIPDoEePTM1PzIBSiyDnpZihdDXKzm8UdXxCDJLUVjBwc1JfBjnaXPEeBKZDuozDss/m98m5qQu+s2Dks000V8cUFTU+BFotzRWX0jWSBpmzse0477b40X2XCPqX0Cqfx9yHdkuMlyF0kJRxXgsGTcwzwbmvqNHJdHHYJJz93hGpBhYMREcDN5VOxXz6Ack3X7xfF29xaC91oOAqq75O11LXF5Y4kAeN9kDG8o6Zsqk4c5at5aqWqzZfnnVtGjhkgU2Mt5aKwptaFMe0Z3ys/zZM4SnsE9NfompnnWsiKk2y09UvrbzuYPWLt43Fp3+IFqRJvBX\" > ~/.ssh/known_hosts' - 'echo -e \"Host *\\n\\tGSSAPIDelegateCredentials yes\\n\\tGSSAPITrustDNS yes\\n\\n\" > ~/.ssh/config' script : - echo \"${SERVICE_ACCOUNT_PASSWORD}\" | kinit -f ${SERVICE_ACCOUNT_USERNAME}@CERN.CH - klist - ssh ${SERVICE_ACCOUNT_USERNAME}@lxplus \"cd PATH_TO_YOUR_AFS && git checkout master && git pull\" only : - master Info Replace PATH_TO_YOUR_AFS with the path to your AFS folder . Do NOT replace SERVICE_ACCOUNT_USERNAME and SERVICE_ACCOUNT_PASSWORD . These will be inserted by gitlab's CI !. More Info on the YAML The .yml was copied and adapted from the acc-models-lhc repository with some changes. In general, a Docker image is loaded, openssh is installed and a Kerberos token is created. With this token it is now possible to ssh to lxplus as the service account, go into the desired directory and checkout the repository. git checkout master is only performed in case someone changed the branch on AFS (which should not happen, do not touch). Image : The image gitlab-registry.cern.ch/linuxsupport/cc7-base used is the default Cern Centos 7 docker image, provided by Linux @ CERN . This is used, because it has Kerberos already set up and configured. The acc-models yaml was using their own docker image , of which the only additional functionality we need is openssh , hence it is installed manually instead. echo lxplus ssh-rsa line: This line adds the public key of the lxplus server to the ssh known_hosts file, so it connects to lxplus without user interaction about this topic (do not touch). echo -e Host line: Here the ssh config is adapted to use Kerberos as authetication method to any server (do not touch). only master : Only the commits to master trigger the CI . Omit this part if you want the repo to be pulled on every commit, or change it to limit upon which commits this happens (as is done in the acc-models yml ). Done ! \u00b6 Whenever you are pushing now any commits to the master branch, the CI / CD will automatically pull the latest commit into the AFS directory.","title":"Setup"},{"location":"resources/howto/setup/#useful-setup-how-tos","text":"","title":"Useful Setup How-To's"},{"location":"resources/howto/setup/#mounting-tn-resources-on-gpn-and-other-machines","text":"To be able to run the GUI or CBNG seamlessly from computers which are not in the technical network, it might be useful to mount /user , /nfs and /eos via sshfs using the following recipe: Create mountpoints and symbolic links (only once) mkdir -p ~/mnt/user && ln -nfs ~/mnt/user /user mkdir ~/mnt/nfs && ln -nfs ~/mnt/nfs /nfs mkdir ~/mnt/eos && ln -nfs ~/mnt/eos /eos Mount network resources (repeat after timeouts and restarts) sshfs username@cs-ccr-dev3.cern.ch:/user/ ~/mnt/user sshfs username@cs-ccr-dev3.cern.ch:/nfs/ ~/mnt/nfs sshfs username@lxplus.cern.ch:/eos/ ~/mnt/eos If outside of the GPN , jump through lxplus to mount dev3 -folders: sshfs username@cs-ccr-dev3.cern.ch:/user/ ~/mnt/user -o ssh_command = 'ssh -t username@lxplus.cern.ch ssh' sshfs username@cs-ccr-dev3.cern.ch:/nfs/ ~/mnt/nfs -o ssh_command = 'ssh -t username@lxplus.cern.ch ssh' To avoid getting asked for your password all the time, you should have your ssh properly configured with Kerberos. In case you need to unmount these sudo fusermount -u ~/mnt/user sudo fusermount -u ~/mnt/nfs sudo fusermount -u ~/mnt/eos","title":"Mounting TN Resources on GPN and Other Machines"},{"location":"resources/howto/setup/#running-guis-locally","text":"To use the KMod GUI or the KnobPanel in the Beta-Beat GUI , it is required to be on the TN , as they need to connect to LSA . If you are in the GPN but not on the TN , you will need to tunnel through some machines. First, install the program sshuttle , which should be available in your package manager. Then, run this command in a terminal and leave it open: sshuttle -vr <username>@cs-ccr-dev2 172 .18.0.0/16 All traffic related to the technical network will be redirected through the cs-ccr-dev2 machine which has access to both networks. In case it isn't available, the other cs-ccr-devX machines can be used.","title":"Running GUIs Locally"},{"location":"resources/howto/setup/#configuring-gitlab-ci-to-automatically-pull-into-afs","text":"If you are programming locally, but also want to have a copy on AFS , either because your colleges are not comfortable with Gitlab or you need the code for other scripts that you are running on lxplus or similar, here is how:","title":"Configuring Gitlab CI to Automatically Pull into AFS"},{"location":"resources/howto/setup/#create-service-account","text":"Security Risk! We will be creating a new account, so you do not have to add the password of your main account to your Gitlab repository : this would be a security risk! Admittedly, it is a low one if everything is done correctly, but just in case something goes wrong or leaks, you can just delete this account . Also, you can give this account only the rights necessary to write to AFS in the first place and do not risk that anyone can access other sensible information. 1. Create a new service account, via the CERN account management , of length 8. You need at least 8 characters to mask that name in Gitlab (if you want to do so), but on the other hand, there is a warning, that AFS cannot handle names longer than 8. So exactly 8 seems to be the sweetspot. If you do not care about masking the name (it is not as important as masking the password, see below) you can go shorter. 2. Set a password of length \u2265 8 containing only [a-zA-Z0-9+/@:] . We want to hide the password later in gitlab, and this is only possible if its >= 8 characters long and only contains Base64 characters as well as @: . You can go as long as you want. Better make this 15 characters long at least. 3. Activate the AFS service for the newly created account.","title":"Create Service Account"},{"location":"resources/howto/setup/#setup-afs-folder","text":"Steps to be done on AFS : 4. Clone your git into your AFS destination. It is important here to you use the right link to set this up, depending on whether you want your repository to be private or public. Go to your Gitlab repository page and click onto the blue clone button on the right. If it is okay for your repo to be publicly accessible you can use the HTTPS url. If you want to set it up privately you will need to clone with the KRB5 url, so Kerberos can be later used to authenticate your service account. ( See also below for giving it rights to the repository.) 5. Run: find . -type d -exec fs sa {} ACCOUNTNAME rlidw \\; Run within the repository or replace . with the repository name! Replace ACCOUNTNAME with the name of your Service Account. This will give read/write access to this repository to the service account. Just giving these rights to the top-folder will not work, as it does not propagate to subfolders automatically. A possible other way would be to create an empty folder and give writing rights to that one to the service account and then login to lxplus with the service account and clone the repository directly with that account. The latter is a good test to see if everything worked correctly anyway. Do not forget, if you have set up the repository with Kerberos authentification, to adapt your .ssh/config to delegate the credentials. Repeated for easy copying: find . -type d -exec fs sa {} ACCOUNTNAME rlidw \\;","title":"Setup AFS folder"},{"location":"resources/howto/setup/#setup-git","text":"Steps to be done on your Gitlab repository: 6. Give your service account access rights or set the repository to public. The former can be done under Members , the latter under Settings \u2192 General \u2192 Visibility, project features, permissions . If you set it up to be a private repository, you need to have cloned the repository with the KRB5 url ( see above ). 7. Add SERVICE_ACCOUNT_USERNAME and SERVICE_ACCOUNT_PASSWORD variables. This is done in Settings \u2192 CI/CD \u2192 Variables . Obviously the values of the variables will be the username and password of your newly created service account respectively. These variables will be used in the next step in the YAML. For obvious reasons, DO NOT WRITE THE PASSWORD IN CLEAR TEXT INTO THE YAML . Security Risk! MAKE SURE AT LEAST THE SERVICE_ACCOUNT_PASSWORD VARIABLE IS SET TO Masked !!! 8. Add a new stage to your .gitlab-ci.yml file, or create a new one. stages : - afs_pull afs : stage : afs_pull image : gitlab-registry.cern.ch/linuxsupport/cc7-base before_script : - yum install -y openssh-clients - mkdir -p ~/.ssh - 'echo \"lxplus ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDTA/5AzXgbkSapknIPDoEePTM1PzIBSiyDnpZihdDXKzm8UdXxCDJLUVjBwc1JfBjnaXPEeBKZDuozDss/m98m5qQu+s2Dks000V8cUFTU+BFotzRWX0jWSBpmzse0477b40X2XCPqX0Cqfx9yHdkuMlyF0kJRxXgsGTcwzwbmvqNHJdHHYJJz93hGpBhYMREcDN5VOxXz6Ack3X7xfF29xaC91oOAqq75O11LXF5Y4kAeN9kDG8o6Zsqk4c5at5aqWqzZfnnVtGjhkgU2Mt5aKwptaFMe0Z3ys/zZM4SnsE9NfompnnWsiKk2y09UvrbzuYPWLt43Fp3+IFqRJvBX\" > ~/.ssh/known_hosts' - 'echo -e \"Host *\\n\\tGSSAPIDelegateCredentials yes\\n\\tGSSAPITrustDNS yes\\n\\n\" > ~/.ssh/config' script : - echo \"${SERVICE_ACCOUNT_PASSWORD}\" | kinit -f ${SERVICE_ACCOUNT_USERNAME}@CERN.CH - klist - ssh ${SERVICE_ACCOUNT_USERNAME}@lxplus \"cd PATH_TO_YOUR_AFS && git checkout master && git pull\" only : - master Info Replace PATH_TO_YOUR_AFS with the path to your AFS folder . Do NOT replace SERVICE_ACCOUNT_USERNAME and SERVICE_ACCOUNT_PASSWORD . These will be inserted by gitlab's CI !. More Info on the YAML The .yml was copied and adapted from the acc-models-lhc repository with some changes. In general, a Docker image is loaded, openssh is installed and a Kerberos token is created. With this token it is now possible to ssh to lxplus as the service account, go into the desired directory and checkout the repository. git checkout master is only performed in case someone changed the branch on AFS (which should not happen, do not touch). Image : The image gitlab-registry.cern.ch/linuxsupport/cc7-base used is the default Cern Centos 7 docker image, provided by Linux @ CERN . This is used, because it has Kerberos already set up and configured. The acc-models yaml was using their own docker image , of which the only additional functionality we need is openssh , hence it is installed manually instead. echo lxplus ssh-rsa line: This line adds the public key of the lxplus server to the ssh known_hosts file, so it connects to lxplus without user interaction about this topic (do not touch). echo -e Host line: Here the ssh config is adapted to use Kerberos as authetication method to any server (do not touch). only master : Only the commits to master trigger the CI . Omit this part if you want the repo to be pulled on every commit, or change it to limit upon which commits this happens (as is done in the acc-models yml ).","title":"Setup Git"},{"location":"resources/howto/setup/#done","text":"Whenever you are pushing now any commits to the master branch, the CI / CD will automatically pull the latest commit into the AFS directory.","title":"Done !"},{"location":"resources/howto/teleworking/","text":"Resources to Ease Teleworking \u00b6 Accessing CERN-internal websites \u00b6 Adapted from here , you can create a web proxy: ssh -D 8090 username@lxtunnel.cern.ch The above command opens a tunnel at port 8090 which can be accessed via browser through localhost:8090 . Accessing Journal Papers etc. \u00b6 Lots of journals and resources can be accessed via the CERN ezproxy by prepending the viewing url with https://ezproxy.cern.ch/login?url= . See this website for a list. Running Graphical Software on lxplus or the TN (e.g. GUI, Eclipse) \u00b6 The most intuitive way to run graphical software on computers within the CERN network from your own PC would be connecting to them by ssh -X and control their GUI via the forwarded X-Server. While this usually works fine from within CERN itself, where connection speeds are high, depending on the ping and bandwidth of your local connection this can be a frustrating experience. Using RDP is a way to avoid this, as with this protocol the graphical interface is rendered locally and only the picture of the screen is transmitted. Sadly, the machine you want to work on ( dev-server , optics-server ) will not have this installed. However, cernts.cern.ch allows you to connect to a windows machine via Remote Desktop Connection from Windows, or e.g. Remmina from Linux. Once logged in with your CERN credentials (Add CERN.CH\\ in front of your username to specify your domain) you can run: Start \u2192 X-Win32 18 \u2192 Lxplus (Default) which opens a putty-terminal connected to lxplus and starts a X-Server in the background. Putty and XServer on cernts Executing any graphical software from this terminal will ask for connection authorization, which you need to approve. Approval prompt for a connection to XServer This way you can run any graphical application smoothly. If your internet connection fails, you should still be able to resume your current session, at least for a while. Creating Shortcut to Other Machines In order to connect to another machine directly (instead of hopping through lxplus ) you can create a shortcut: Open X-Win32 18 Configuration . Manual... \u2192 More... \u2192 command . Target: \"C:\\Program Files (x86)\\PuTTY_CERN\\putty.exe\" -ssh -X machine_at_cern.cern.ch . Fill out the other fields to your liking. You can even create a shortcut on the screen, from the right-click menu on the newly created connection. Alternative way to create configuration Create a file with the ending .xw32 , e.g on the desktop. Fill it with: <Session> <HideOnStart> false </HideOnStart> <Module> command </Module> <Name> name_you_want_to_give </Name> <NewInstance> never </NewInstance> <Settings> <Target> \"C:\\Program Files (x86)\\PuTTY_CERN\\putty.exe\" -ssh -X machine_at_cern.cern.ch </Target> </Settings> <ShowStatus> false </ShowStatus> <WindowMode> multiple </WindowMode> </Session> Click on file.","title":"Teleworking"},{"location":"resources/howto/teleworking/#resources-to-ease-teleworking","text":"","title":"Resources to Ease Teleworking"},{"location":"resources/howto/teleworking/#accessing-cern-internal-websites","text":"Adapted from here , you can create a web proxy: ssh -D 8090 username@lxtunnel.cern.ch The above command opens a tunnel at port 8090 which can be accessed via browser through localhost:8090 .","title":"Accessing CERN-internal websites"},{"location":"resources/howto/teleworking/#accessing-journal-papers-etc","text":"Lots of journals and resources can be accessed via the CERN ezproxy by prepending the viewing url with https://ezproxy.cern.ch/login?url= . See this website for a list.","title":"Accessing Journal Papers etc."},{"location":"resources/howto/teleworking/#running-graphical-software-on-lxplus-or-the-tn-eg-gui-eclipse","text":"The most intuitive way to run graphical software on computers within the CERN network from your own PC would be connecting to them by ssh -X and control their GUI via the forwarded X-Server. While this usually works fine from within CERN itself, where connection speeds are high, depending on the ping and bandwidth of your local connection this can be a frustrating experience. Using RDP is a way to avoid this, as with this protocol the graphical interface is rendered locally and only the picture of the screen is transmitted. Sadly, the machine you want to work on ( dev-server , optics-server ) will not have this installed. However, cernts.cern.ch allows you to connect to a windows machine via Remote Desktop Connection from Windows, or e.g. Remmina from Linux. Once logged in with your CERN credentials (Add CERN.CH\\ in front of your username to specify your domain) you can run: Start \u2192 X-Win32 18 \u2192 Lxplus (Default) which opens a putty-terminal connected to lxplus and starts a X-Server in the background. Putty and XServer on cernts Executing any graphical software from this terminal will ask for connection authorization, which you need to approve. Approval prompt for a connection to XServer This way you can run any graphical application smoothly. If your internet connection fails, you should still be able to resume your current session, at least for a while. Creating Shortcut to Other Machines In order to connect to another machine directly (instead of hopping through lxplus ) you can create a shortcut: Open X-Win32 18 Configuration . Manual... \u2192 More... \u2192 command . Target: \"C:\\Program Files (x86)\\PuTTY_CERN\\putty.exe\" -ssh -X machine_at_cern.cern.ch . Fill out the other fields to your liking. You can even create a shortcut on the screen, from the right-click menu on the newly created connection. Alternative way to create configuration Create a file with the ending .xw32 , e.g on the desktop. Fill it with: <Session> <HideOnStart> false </HideOnStart> <Module> command </Module> <Name> name_you_want_to_give </Name> <NewInstance> never </NewInstance> <Settings> <Target> \"C:\\Program Files (x86)\\PuTTY_CERN\\putty.exe\" -ssh -X machine_at_cern.cern.ch </Target> </Settings> <ShowStatus> false </ShowStatus> <WindowMode> multiple </WindowMode> </Session> Click on file.","title":"Running Graphical Software on lxplus or the TN (e.g. GUI, Eclipse)"},{"location":"resources/howto/wiki/","text":"Editing Wiki pages \u00b6 This page presents an overview of the steps necessary to modify the wiki and add new pages. It also contains examples for useful commands and can be used as a template for new pages. More detailed information can be found here: markdown guide mkdocs homepage mkdocs material help Setup \u00b6 To Make Changes Online \u00b6 This is a method adapted to making small changes, most likely to a single page. Go to the current version of the site here , navigate to a page you wish to modify and click the page's modification link . You will be taken to the Github GUI to make your changes, which you can later commit. Even new pages can be added directly online on github, by navigating to the desired location and clicking on the Add File button. See the section about adding pages for more info about what is required for new pages to be accessible from the menu. Note As a lot of fancy styling is added by the material theme used, only the basic markdown formatting is seen in the preview. To Make Changes Locally \u00b6 For bigger changes, local development is recommended. Get a local copy of this repository, set up a Python3.6+ environment and install the dependencies: git clone https://github.com/pylhc/pylhc.github.io pip install mkdocs mkdocs-material mkdocs-minify-plugin Create a branch (from master) and make your changes. You can run a local server by running python -m mkdocs serve from the top-level directory, and see the site rendered locally in your browser at localhost:8000 . The rendered website will automatically reload upon changes to any file located in the docs directory. Commit your changes to this repository, and open a pull request to get them approved once they are ready. Adding a page \u00b6 In order to add a new page, a new .md should be created in the appropriate location in the folder structure. A link to the page then needs to be added in the nav section of the mkdocs.yml in the root directory, together with an ID. Guidelines \u00b6 Pages are written in Markdown (file extension .md ). A general overview of the syntax a well as some best practices can be found here . Additionally, to allow for easier comparison between two versions of a file, it is recommended to keep it to one sentence per line. Ideally, the line length is kept below 100 characters. Following this, it is also recommended to not put links in the text, instead creating an ID at the end of the document and linking to this. Different blocks of either code or text should be separated by one blank line. To create blocks of code, use fenced code blocks , which are created using ``` . These blocks of code should be separated from the previous and following text by one blank line. To allow for syntax highlighting, the language should be specified. Below a basic example. ```bash something code that does something ``` Environments \u00b6 General text formatting \u00b6 These are basic markdown commands, repeated here for convenience. A more exhaustive list is available here . To create: bold text, surround the text with **text** . italic text, surround the text with _text_ . an internal link , use [link](../howto/wiki.md#general-text-formatting) Note that all links are relative to the current document! The # -labels are created by headers automatically and can be omitted, in case you want to link to the page itself. an external link , use [link][bestwiki]{target=_blank} . Note that at the bottom of the file, an ID named bestwiki should be created, together with the hyperlink to the webpage, like so: [bestwiki]: https://pylhc.github.io/ The specifier {target=_blank} is added to ensure pages open in a new tab. In the specifier, additional information on the accessibility can be added. Links accessable only with a CERN login can be marked like {target=_blank .cern_login} or from the CERN network like {target=_blank .cern_internal} . quote such as the one just above: > to markdown, or not to markdown small hints to a difficult word , which appear on mouse over, add at the bottom of the file: *[difficult word]: helpful explanation Code listing \u00b6 To highlight code inline, surround the text to highlight with ` . For creating a code block, surround the code with ``` . Please make sure that the code block is separated from the text by one blank line. Additionally, please specify the language. ```python for i in range(3): print(i) ``` Lists \u00b6 Unordered list An unordered list looks like this. First Second Third It can be created using - First - Second - Third Ordered list An ordered list looks like this. First Second Third It can be created using 1. First 2. Second 3. Third Task list A task list looks like this. First Second Third It can be created using - [ ] First - [x] Second - [ ] Third Procedure Task List Below, a task list with hints, as is used in the procedures, is displayed. Task 1 Summary Hints for task 1. Task 2 Summary Hints for task 2. Task 3 Summary Hints for task 3. It can be created using the following code. - [ ] < details class = \"nodeco\" >< summary > Task 1 Summary </ summary > < p > Hints for task 1. </ p ></ details > - [ ] < details class = \"nodeco\" >< summary > Task 2 Summary </ summary > < p > Hints for task 2. </ p ></ details > - [ ] < details class = \"nodeco\" >< summary > Task 3 Summary </ summary > < p > Hints for task 3. </ p ></ details > Note that ticks set by a user are not permanent and will be reset upon reloading the page. Tabbed List The code to create a tabbed list, like the one we are in right now: === \"Entry 1\" Text 1 === \"Entry 2\" Text 2 === \"Entry 3\" Text 3 Text Boxes \u00b6 Paper Box Textbox for adding papers. Code: ??? abstract \"Paper Box\" Textbox for adding papers. Info Box Textbox for adding useful information. Code: ??? info \"Info Box\" Textbox for adding useful information. Question Box Textbox for answering commonly asked questions. Code: ??? question \"Question Box\" Textbox for answering commonly asked questions. Tip Box Textbox for adding useful tips to novice users. Code: ??? tip \"Tip Box\" Textbox for adding useful tips to novice users. Note that appending a + to ??? will result in an expanded box by default. Note Box Textbox for adding small notes. Code: !!! note \"Note Box\" Textbox for adding small notes. Warning Box Textbox for warning users of common pitfalls. Code: !!! warning \"Warning Box\" Textbox for warning users of common pitfalls. Danger Box Textbox for warning users of potential serious consequences if not executed properly. Code: !!! danger \"Danger Box\" Textbox for warning users of potential serious consequences if not executed properly. Todo Box Textbox for warning users that this webpage is still work in progress. Code: !!! todo \"Todo Box\" Textbox for warning users that this webpage is still work in progress. Various other commands \u00b6 Tables \u00b6 Tables in markdown look like this. Column 1 Column 2 Entry 11 Entry 21 Entry 12 Entry 22 It can be created using | Column 1 | Column 2 | | ----------- | ----------- | | Entry 11 | Entry 21 | | Entry 12 | Entry 22 | When creating a table, please ensure that column width is constant and that pipes ( | ) are aligned. Images \u00b6 To paste an image, use the following code. ![Image](../../assets/images/tricks/placeholder.gif) To include a centered image with a caption, use the following code. < figure > < img src = \"../../../assets/images/something/image.png\" width = 90% > < figcaption > Figure: Something really amazing. </ figcaption > </ figure > Images should be saved in assets/images , in an appropriately named folder.","title":"Editing Wiki"},{"location":"resources/howto/wiki/#editing-wiki-pages","text":"This page presents an overview of the steps necessary to modify the wiki and add new pages. It also contains examples for useful commands and can be used as a template for new pages. More detailed information can be found here: markdown guide mkdocs homepage mkdocs material help","title":"Editing Wiki pages"},{"location":"resources/howto/wiki/#setup","text":"","title":"Setup"},{"location":"resources/howto/wiki/#to-make-changes-online","text":"This is a method adapted to making small changes, most likely to a single page. Go to the current version of the site here , navigate to a page you wish to modify and click the page's modification link . You will be taken to the Github GUI to make your changes, which you can later commit. Even new pages can be added directly online on github, by navigating to the desired location and clicking on the Add File button. See the section about adding pages for more info about what is required for new pages to be accessible from the menu. Note As a lot of fancy styling is added by the material theme used, only the basic markdown formatting is seen in the preview.","title":"To Make Changes Online"},{"location":"resources/howto/wiki/#to-make-changes-locally","text":"For bigger changes, local development is recommended. Get a local copy of this repository, set up a Python3.6+ environment and install the dependencies: git clone https://github.com/pylhc/pylhc.github.io pip install mkdocs mkdocs-material mkdocs-minify-plugin Create a branch (from master) and make your changes. You can run a local server by running python -m mkdocs serve from the top-level directory, and see the site rendered locally in your browser at localhost:8000 . The rendered website will automatically reload upon changes to any file located in the docs directory. Commit your changes to this repository, and open a pull request to get them approved once they are ready.","title":"To Make Changes Locally"},{"location":"resources/howto/wiki/#adding-a-page","text":"In order to add a new page, a new .md should be created in the appropriate location in the folder structure. A link to the page then needs to be added in the nav section of the mkdocs.yml in the root directory, together with an ID.","title":"Adding a page"},{"location":"resources/howto/wiki/#guidelines","text":"Pages are written in Markdown (file extension .md ). A general overview of the syntax a well as some best practices can be found here . Additionally, to allow for easier comparison between two versions of a file, it is recommended to keep it to one sentence per line. Ideally, the line length is kept below 100 characters. Following this, it is also recommended to not put links in the text, instead creating an ID at the end of the document and linking to this. Different blocks of either code or text should be separated by one blank line. To create blocks of code, use fenced code blocks , which are created using ``` . These blocks of code should be separated from the previous and following text by one blank line. To allow for syntax highlighting, the language should be specified. Below a basic example. ```bash something code that does something ```","title":"Guidelines"},{"location":"resources/howto/wiki/#environments","text":"","title":"Environments"},{"location":"resources/howto/wiki/#general-text-formatting","text":"These are basic markdown commands, repeated here for convenience. A more exhaustive list is available here . To create: bold text, surround the text with **text** . italic text, surround the text with _text_ . an internal link , use [link](../howto/wiki.md#general-text-formatting) Note that all links are relative to the current document! The # -labels are created by headers automatically and can be omitted, in case you want to link to the page itself. an external link , use [link][bestwiki]{target=_blank} . Note that at the bottom of the file, an ID named bestwiki should be created, together with the hyperlink to the webpage, like so: [bestwiki]: https://pylhc.github.io/ The specifier {target=_blank} is added to ensure pages open in a new tab. In the specifier, additional information on the accessibility can be added. Links accessable only with a CERN login can be marked like {target=_blank .cern_login} or from the CERN network like {target=_blank .cern_internal} . quote such as the one just above: > to markdown, or not to markdown small hints to a difficult word , which appear on mouse over, add at the bottom of the file: *[difficult word]: helpful explanation","title":"General text formatting"},{"location":"resources/howto/wiki/#code-listing","text":"To highlight code inline, surround the text to highlight with ` . For creating a code block, surround the code with ``` . Please make sure that the code block is separated from the text by one blank line. Additionally, please specify the language. ```python for i in range(3): print(i) ```","title":"Code listing"},{"location":"resources/howto/wiki/#lists","text":"Unordered list An unordered list looks like this. First Second Third It can be created using - First - Second - Third Ordered list An ordered list looks like this. First Second Third It can be created using 1. First 2. Second 3. Third Task list A task list looks like this. First Second Third It can be created using - [ ] First - [x] Second - [ ] Third Procedure Task List Below, a task list with hints, as is used in the procedures, is displayed. Task 1 Summary Hints for task 1. Task 2 Summary Hints for task 2. Task 3 Summary Hints for task 3. It can be created using the following code. - [ ] < details class = \"nodeco\" >< summary > Task 1 Summary </ summary > < p > Hints for task 1. </ p ></ details > - [ ] < details class = \"nodeco\" >< summary > Task 2 Summary </ summary > < p > Hints for task 2. </ p ></ details > - [ ] < details class = \"nodeco\" >< summary > Task 3 Summary </ summary > < p > Hints for task 3. </ p ></ details > Note that ticks set by a user are not permanent and will be reset upon reloading the page. Tabbed List The code to create a tabbed list, like the one we are in right now: === \"Entry 1\" Text 1 === \"Entry 2\" Text 2 === \"Entry 3\" Text 3","title":"Lists"},{"location":"resources/howto/wiki/#text-boxes","text":"Paper Box Textbox for adding papers. Code: ??? abstract \"Paper Box\" Textbox for adding papers. Info Box Textbox for adding useful information. Code: ??? info \"Info Box\" Textbox for adding useful information. Question Box Textbox for answering commonly asked questions. Code: ??? question \"Question Box\" Textbox for answering commonly asked questions. Tip Box Textbox for adding useful tips to novice users. Code: ??? tip \"Tip Box\" Textbox for adding useful tips to novice users. Note that appending a + to ??? will result in an expanded box by default. Note Box Textbox for adding small notes. Code: !!! note \"Note Box\" Textbox for adding small notes. Warning Box Textbox for warning users of common pitfalls. Code: !!! warning \"Warning Box\" Textbox for warning users of common pitfalls. Danger Box Textbox for warning users of potential serious consequences if not executed properly. Code: !!! danger \"Danger Box\" Textbox for warning users of potential serious consequences if not executed properly. Todo Box Textbox for warning users that this webpage is still work in progress. Code: !!! todo \"Todo Box\" Textbox for warning users that this webpage is still work in progress.","title":"Text Boxes"},{"location":"resources/howto/wiki/#various-other-commands","text":"","title":"Various other commands"},{"location":"resources/howto/wiki/#tables","text":"Tables in markdown look like this. Column 1 Column 2 Entry 11 Entry 21 Entry 12 Entry 22 It can be created using | Column 1 | Column 2 | | ----------- | ----------- | | Entry 11 | Entry 21 | | Entry 12 | Entry 22 | When creating a table, please ensure that column width is constant and that pipes ( | ) are aligned.","title":"Tables"},{"location":"resources/howto/wiki/#images","text":"To paste an image, use the following code. ![Image](../../assets/images/tricks/placeholder.gif) To include a centered image with a caption, use the following code. < figure > < img src = \"../../../assets/images/something/image.png\" width = 90% > < figcaption > Figure: Something really amazing. </ figcaption > </ figure > Images should be saved in assets/images , in an appropriately named folder.","title":"Images"}]}